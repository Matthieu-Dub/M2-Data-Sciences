{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpPHz2Lp6VD"
      },
      "source": [
        "# TP Coding an autoencoder in Pytorch\n",
        "\n",
        "Author : Alasdair Newson\n",
        "\n",
        "alasdair.newson@telecom-paris.fr\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH9YkFsrsidu"
      },
      "source": [
        "![AUTOENCODER](https://perso.telecom-paristech.fr/anewson/doc/images/autoencoder_illustration_2.png)\n",
        "\n",
        "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
        "\n",
        "### Your task:\n",
        "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE)\n",
        "\n",
        "\n",
        "First of all, let's load some packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JqNeIJ8Op8Ao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def pytorch_to_numpy(x):\n",
        "  return x.detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyj5dj_eui9D"
      },
      "source": [
        "Now, we load the MNIST dataset, which we will use for training a simple autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4YPLKlPrufSk"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset\n",
        "mnist_trainset = datasets.MNIST(root='mnist_data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_testset = datasets.MNIST(root='mnist_data', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "#create data loader with smaller dataset size\n",
        "max_mnist_size = 1000\n",
        "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=256, shuffle=True,drop_last=True)\n",
        "\n",
        "# download test dataset\n",
        "max_mnist_size = 512\n",
        "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0] \n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=256, shuffle=True,drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "r7YhlBT2PN9I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mnist_trainset_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bkK4ktwfvC"
      },
      "source": [
        "# 1 Vanilla Autoencoder\n",
        "\n",
        "Now, we define the general parameters of our autoencoder. In particular, we note that the latent space is of size 10. We have chosen this since there are 10 digits, but you can change this as you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mD56EDzbvUxq"
      },
      "outputs": [],
      "source": [
        "# autoencoder parameters\n",
        "n_rows = mnist_trainset_reduced.dataset.train_data.shape[1]\n",
        "n_cols = mnist_trainset_reduced.dataset.train_data.shape[2]\n",
        "n_channels = 1\n",
        "n_pixels = n_rows*n_cols\n",
        "\n",
        "img_shape = (n_rows, n_cols, n_channels)\n",
        "z_dim = 25\n",
        "n_epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLa2-jQwxSI"
      },
      "source": [
        "Now, define the autoencoder architecture by creating a Pytorch class. We will use the following MLP architecture :\n",
        "\n",
        "Encoder :\n",
        "- Flatten input\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer (no non-linearity), output size ```z_dim```\n",
        "\n",
        "Decoder :\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ Sigmoid Activation\n",
        "- Reshape, to size $28\\times 28\\times 1$\n",
        "\n",
        "The intermediate dimensions are referred to as ```h_dim_1``` and ```h_dim2```. For the reshape operation (and even the flatten), you can use the __view__ operation of Pytorch, for example:\n",
        "- ```x.view(-1,n_channels, n_rows,n_cols)```\n",
        "\n",
        "You can use the ```F.relu()``` function for the ReLU non-linearity (or any other choice you like)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tEuZfnUlXxMl"
      },
      "outputs": [],
      "source": [
        "class AE(torch.nn.Module ):\n",
        "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
        "    super(AE, self).__init__()\n",
        "\n",
        "    self.n_rows = n_rows\n",
        "    self.n_cols = n_cols\n",
        "    self.n_channels = n_channels\n",
        "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    # encoder part\n",
        "    self.fc1 = nn.Linear(x_dim, h_dim1) # FILL IN STUDENT \n",
        "    self.fc2 = nn.Linear(h_dim1, h_dim2) # FILL IN STUDENT \n",
        "    self.fc3 = nn.Linear(h_dim2, z_dim) # FILL IN STUDENT \n",
        "    # decoder part\n",
        "    self.fc4 = nn.Linear(z_dim, h_dim2) # FILL IN STUDENT \n",
        "    self.fc5 = nn.Linear(h_dim2, h_dim1) # FILL IN STUDENT \n",
        "    self.fc6 = nn.Linear(h_dim1, x_dim) # FILL IN STUDENT \n",
        "\n",
        "  def encoder(self, x):\n",
        "    h = F.relu(self.fc1(x.view(x.size(0), -1)))  #FILL IN STUDENT \n",
        "    h = F.relu(self.fc2(h)) # FILL IN STUDENT \n",
        "    return self.fc3(h) # FILL IN STUDENT \n",
        "  def decoder(self, z):\n",
        "    h = F.relu(self.fc4(z)) # FILL IN STUDENT \n",
        "    h = F.relu(self.fc5(h)) # FILL IN STUDENT \n",
        "    return torch.sigmoid(self.fc6(h)).view(-1,n_channels, n_rows,n_cols) # FILL IN STUDENT \n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.decoder(self.encoder(x)) # FILL IN STUDENT \n",
        "    return(y)\n",
        "    \n",
        "  def loss_function(self,x, y):\n",
        "    bce_loss = F.binary_cross_entropy(x, y) # FILL IN STUDENT \n",
        "    return torch.mean(bce_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oV40vRMQRoG1"
      },
      "outputs": [],
      "source": [
        "# create an instance of the model\n",
        "ae_dim_1 = 512\n",
        "ae_dim_2 = 256\n",
        "ae_model = AE(x_dim=n_pixels, h_dim1= ae_dim_1, h_dim2=ae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
        "ae_optimizer = optim.Adam(ae_model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-659aM36xvXX"
      },
      "source": [
        "Now, define a generic function to train the model for one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wfqX6Brlxjyi"
      },
      "outputs": [],
      "source": [
        "def train_ae(ae_model,data_train_loader,epoch):\n",
        "\ttrain_loss = 0\n",
        "\tfor batch_idx, (data, _) in enumerate(data_train_loader):\n",
        "\t\tae_optimizer.zero_grad()\n",
        "\t\t\n",
        "\t\ty = ae_model.forward(data) # FILL IN STUDENT \n",
        "\t\tloss_ae = ae_model.loss_function(y, data) # FILL IN STUDENT \n",
        "  \n",
        "\t\tloss_ae.backward()\n",
        "\t\ttrain_loss += loss_ae.item()\n",
        "\t\tae_optimizer.step()\n",
        "\t\t\n",
        "\t\tif batch_idx % 100 == 0:\n",
        "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "\t\t\t\tepoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
        "\t\t\t\t100. * batch_idx / len(data_train_loader), loss_ae.item() / len(data)))\n",
        "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EbmswSzJdK"
      },
      "source": [
        "Finally, train the model for ```n_epochs``` epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "11q_0PSibZk-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 0.002707\n",
            "====> Epoch: 0 Average loss: 0.0021\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 0.002700\n",
            "====> Epoch: 1 Average loss: 0.0021\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 0.002691\n",
            "====> Epoch: 2 Average loss: 0.0021\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 0.002682\n",
            "====> Epoch: 3 Average loss: 0.0021\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 0.002669\n",
            "====> Epoch: 4 Average loss: 0.0020\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 0.002653\n",
            "====> Epoch: 5 Average loss: 0.0020\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 0.002632\n",
            "====> Epoch: 6 Average loss: 0.0020\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 0.002603\n",
            "====> Epoch: 7 Average loss: 0.0020\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 0.002563\n",
            "====> Epoch: 8 Average loss: 0.0020\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 0.002505\n",
            "====> Epoch: 9 Average loss: 0.0019\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 0.002428\n",
            "====> Epoch: 10 Average loss: 0.0018\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 0.002318\n",
            "====> Epoch: 11 Average loss: 0.0017\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 0.002180\n",
            "====> Epoch: 12 Average loss: 0.0016\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 0.002004\n",
            "====> Epoch: 13 Average loss: 0.0015\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 0.001813\n",
            "====> Epoch: 14 Average loss: 0.0013\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 0.001619\n",
            "====> Epoch: 15 Average loss: 0.0012\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 0.001478\n",
            "====> Epoch: 16 Average loss: 0.0011\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 0.001350\n",
            "====> Epoch: 17 Average loss: 0.0010\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 0.001316\n",
            "====> Epoch: 18 Average loss: 0.0010\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 0.001256\n",
            "====> Epoch: 19 Average loss: 0.0010\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 0.001238\n",
            "====> Epoch: 20 Average loss: 0.0009\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 0.001222\n",
            "====> Epoch: 21 Average loss: 0.0009\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 0.001164\n",
            "====> Epoch: 22 Average loss: 0.0009\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 0.001139\n",
            "====> Epoch: 23 Average loss: 0.0009\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 0.001141\n",
            "====> Epoch: 24 Average loss: 0.0009\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 0.001144\n",
            "====> Epoch: 25 Average loss: 0.0009\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 0.001091\n",
            "====> Epoch: 26 Average loss: 0.0009\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 0.001134\n",
            "====> Epoch: 27 Average loss: 0.0008\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 0.001101\n",
            "====> Epoch: 28 Average loss: 0.0008\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 0.001086\n",
            "====> Epoch: 29 Average loss: 0.0008\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 0.001081\n",
            "====> Epoch: 30 Average loss: 0.0008\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLoss: 0.001062\n",
            "====> Epoch: 31 Average loss: 0.0008\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLoss: 0.001059\n",
            "====> Epoch: 32 Average loss: 0.0008\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLoss: 0.001066\n",
            "====> Epoch: 33 Average loss: 0.0008\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLoss: 0.001088\n",
            "====> Epoch: 34 Average loss: 0.0008\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLoss: 0.001078\n",
            "====> Epoch: 35 Average loss: 0.0008\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLoss: 0.001079\n",
            "====> Epoch: 36 Average loss: 0.0008\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLoss: 0.001051\n",
            "====> Epoch: 37 Average loss: 0.0008\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLoss: 0.001046\n",
            "====> Epoch: 38 Average loss: 0.0008\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLoss: 0.001048\n",
            "====> Epoch: 39 Average loss: 0.0008\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLoss: 0.001034\n",
            "====> Epoch: 40 Average loss: 0.0008\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLoss: 0.001057\n",
            "====> Epoch: 41 Average loss: 0.0008\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLoss: 0.001053\n",
            "====> Epoch: 42 Average loss: 0.0008\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLoss: 0.001056\n",
            "====> Epoch: 43 Average loss: 0.0008\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLoss: 0.001046\n",
            "====> Epoch: 44 Average loss: 0.0008\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLoss: 0.001051\n",
            "====> Epoch: 45 Average loss: 0.0008\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLoss: 0.001040\n",
            "====> Epoch: 46 Average loss: 0.0008\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLoss: 0.001031\n",
            "====> Epoch: 47 Average loss: 0.0008\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLoss: 0.001046\n",
            "====> Epoch: 48 Average loss: 0.0008\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLoss: 0.001034\n",
            "====> Epoch: 49 Average loss: 0.0008\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLoss: 0.001042\n",
            "====> Epoch: 50 Average loss: 0.0008\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLoss: 0.001017\n",
            "====> Epoch: 51 Average loss: 0.0008\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLoss: 0.001031\n",
            "====> Epoch: 52 Average loss: 0.0008\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLoss: 0.001031\n",
            "====> Epoch: 53 Average loss: 0.0008\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLoss: 0.000995\n",
            "====> Epoch: 54 Average loss: 0.0008\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLoss: 0.001014\n",
            "====> Epoch: 55 Average loss: 0.0008\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLoss: 0.001042\n",
            "====> Epoch: 56 Average loss: 0.0008\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLoss: 0.001007\n",
            "====> Epoch: 57 Average loss: 0.0008\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLoss: 0.001013\n",
            "====> Epoch: 58 Average loss: 0.0008\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLoss: 0.001010\n",
            "====> Epoch: 59 Average loss: 0.0008\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLoss: 0.001017\n",
            "====> Epoch: 60 Average loss: 0.0008\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLoss: 0.001018\n",
            "====> Epoch: 61 Average loss: 0.0008\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLoss: 0.001022\n",
            "====> Epoch: 62 Average loss: 0.0008\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLoss: 0.001028\n",
            "====> Epoch: 63 Average loss: 0.0008\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLoss: 0.001016\n",
            "====> Epoch: 64 Average loss: 0.0008\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLoss: 0.001024\n",
            "====> Epoch: 65 Average loss: 0.0008\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLoss: 0.001026\n",
            "====> Epoch: 66 Average loss: 0.0008\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLoss: 0.001015\n",
            "====> Epoch: 67 Average loss: 0.0008\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLoss: 0.001025\n",
            "====> Epoch: 68 Average loss: 0.0008\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLoss: 0.001015\n",
            "====> Epoch: 69 Average loss: 0.0008\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLoss: 0.001012\n",
            "====> Epoch: 70 Average loss: 0.0008\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLoss: 0.000996\n",
            "====> Epoch: 71 Average loss: 0.0008\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLoss: 0.001010\n",
            "====> Epoch: 72 Average loss: 0.0008\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLoss: 0.000984\n",
            "====> Epoch: 73 Average loss: 0.0008\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLoss: 0.001009\n",
            "====> Epoch: 74 Average loss: 0.0008\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLoss: 0.001009\n",
            "====> Epoch: 75 Average loss: 0.0008\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLoss: 0.000993\n",
            "====> Epoch: 76 Average loss: 0.0008\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLoss: 0.000992\n",
            "====> Epoch: 77 Average loss: 0.0008\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLoss: 0.000998\n",
            "====> Epoch: 78 Average loss: 0.0008\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLoss: 0.000986\n",
            "====> Epoch: 79 Average loss: 0.0008\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLoss: 0.000986\n",
            "====> Epoch: 80 Average loss: 0.0008\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLoss: 0.000996\n",
            "====> Epoch: 81 Average loss: 0.0008\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLoss: 0.000972\n",
            "====> Epoch: 82 Average loss: 0.0008\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLoss: 0.000966\n",
            "====> Epoch: 83 Average loss: 0.0008\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLoss: 0.000963\n",
            "====> Epoch: 84 Average loss: 0.0008\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLoss: 0.000964\n",
            "====> Epoch: 85 Average loss: 0.0008\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLoss: 0.000985\n",
            "====> Epoch: 86 Average loss: 0.0008\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLoss: 0.000998\n",
            "====> Epoch: 87 Average loss: 0.0008\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLoss: 0.000966\n",
            "====> Epoch: 88 Average loss: 0.0007\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLoss: 0.000989\n",
            "====> Epoch: 89 Average loss: 0.0007\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLoss: 0.000982\n",
            "====> Epoch: 90 Average loss: 0.0007\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLoss: 0.000975\n",
            "====> Epoch: 91 Average loss: 0.0007\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLoss: 0.000972\n",
            "====> Epoch: 92 Average loss: 0.0007\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLoss: 0.000961\n",
            "====> Epoch: 93 Average loss: 0.0007\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLoss: 0.000962\n",
            "====> Epoch: 94 Average loss: 0.0007\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLoss: 0.000968\n",
            "====> Epoch: 95 Average loss: 0.0007\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLoss: 0.000963\n",
            "====> Epoch: 96 Average loss: 0.0007\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLoss: 0.000935\n",
            "====> Epoch: 97 Average loss: 0.0007\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLoss: 0.000953\n",
            "====> Epoch: 98 Average loss: 0.0007\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLoss: 0.000971\n",
            "====> Epoch: 99 Average loss: 0.0007\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(0, n_epochs):\n",
        "  train_ae(ae_model,mnist_train_loader,epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "T8jXjdRyzMy2"
      },
      "outputs": [],
      "source": [
        "# some functions to display images and autoencoded images\n",
        "\n",
        "def display_images(imgs):\n",
        "  \n",
        "  r = 1\n",
        "  c = imgs.shape[0]\n",
        "  fig = plt.figure(figsize=(15, 5))\n",
        "  axs = fig.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    #black and white images\n",
        "    axs[j].imshow(pytorch_to_numpy(imgs[j, 0,:,:]), cmap='gray')\n",
        "    axs[j].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def display_ae_images(ae_model, test_imgs):\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "  print(test_imgs.shape)\n",
        "\n",
        "  #get output images\n",
        "  output_imgs = pytorch_to_numpy(ae_model.forward( test_imgs ))\n",
        "  print(output_imgs.shape)\n",
        "  \n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    #black and white images\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9pbXch29d68D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 1, 28, 28])\n",
            "(5, 1, 28, 28)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(ae_model, test_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP12L-DI12lQ"
      },
      "source": [
        "Do these autoencoded images look good to you ? You can try to change the latent space size to see if the results improve. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UqeNhuSdnDt"
      },
      "source": [
        "# 2/ Variational autoencoder\n",
        "\n",
        "Now, we are going to create an variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder\n",
        "\n",
        "## Main idea\n",
        "\n",
        "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools : \n",
        "\n",
        "- A specific architecture, where the encoder produces the average and variance of the latent codes\n",
        "- A specially designed loss function\n",
        "\n",
        "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The architecture of the VAE model is as follows:\n",
        "\n",
        "The encoder consists of:\n",
        "\n",
        "Encoder :\n",
        "- Flatten input\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer (no non-linarity) to produce the average, Dense layer (no non-linarity) to produce the variance (these last two layers are in parallel)\n",
        "\n",
        "Decoder :\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ ReLU\n",
        "- Dense layer $+$ Sigmoid Activation\n",
        "- Reshape, to size $28\\times 28\\times 1$\n",
        "\n",
        "\n",
        "## Variational Autoencoder loss\n",
        "\n",
        "Recall that for the VAE, the loss function is in fact a function to __maximise__. In fact, for implementation, you will see that it is easier to __minimise__ $-\\mathcal{L}$.\n",
        "\n",
        "In the case of an image which is represented by a set of __Bernoulli__ variables (which is relevant for mnist), the original loss function (to maximise) is written :\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L} &= \\log\\left(p_\\theta(x|z)\\right) - KL\\left( q_\\phi(z|x) \\; || \\; p_\\theta(z)\\right) \\\\\n",
        "    &= \\left(\\sum_{i} x_i \\log y_i + (1-x_i) \\log (1-y_i)\\right) - \\left(\\frac{1}{2} \\sum_j \\left( \\sigma_j^2 + \\mu_j^2 - \\log \\sigma_j^2 -1 \\right)\\right)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "where $i$ is summed over the image pixels, and $j$ is summed over the elements of the latent space. $\\sigma_j^2$ is the $j$th element of the latent space variance, and $\\mu_j$ is the $j$th element of the latent space mean.\n",
        "\n",
        "The left part of the loss (reconstruction error) can be implemented simply as the binary cross-entropy between the input x and the output y. Since we are __maximising__ $-$[binary cross-entropy] (look at the formula), this is equivalent to minimising the binary cross-entropy.\n",
        "\n",
        "For the right part of the equation (KL divergence), you need to implement it manually. \n",
        "\n",
        "The final loss is the average, over the batch size, of the sum of the reconstruction error (left part) and the KL divergence (right part). Be careful, in the formula, the sums over $i$ and $j$ are over the number of pixels and the number of latent elements, respectively. To achieve a sum rather than an average, you can use ```torch.nn.BCELoss(reduction='sum')()```, and the ```torch.sum()``` functions.\n",
        "\n",
        "As in the case of the normal autoencoder, you will need to flatten and then reshape the tensors at the beginning/end of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6siMHQLheM4T"
      },
      "outputs": [],
      "source": [
        "class VAE(torch.nn.Module ):\n",
        "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.n_rows = n_rows\n",
        "    self.n_cols = n_cols\n",
        "    self.n_channels = n_channels\n",
        "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    # encoder part\n",
        "    self.fc1 = torch.nn.Linear(self.n_pixels, h_dim1) #FILL IN STUDENT\n",
        "    self.fc2 = torch.nn.Linear(h_dim1, h_dim2) #FILL IN STUDENT\n",
        "    self.fc31 = torch.nn.Linear(h_dim2, z_dim) #FILL IN STUDENT\n",
        "    self.fc32 = torch.nn.Linear(h_dim2, z_dim) #FILL IN STUDENT\n",
        "    # decoder part\n",
        "    self.fc4 = torch.nn.Linear(z_dim, h_dim2) #FILL IN STUDENT\n",
        "    self.fc5 = torch.nn.Linear(h_dim2, h_dim1) #FILL IN STUDENT\n",
        "    self.fc6 = torch.nn.Linear(h_dim1, self.n_pixels) #FILL IN STUDENT\n",
        "\n",
        "  def encoder(self, x):\n",
        "    h = torch.relu(self.fc1(x)) #FILL IN STUDENT\n",
        "    h = torch.relu(self.fc2(h)) #FILL IN STUDENT\n",
        "    return self.fc31(h), self.fc32(h) #FILL IN STUDENT (remember, there are two outputs)\n",
        "  def decoder(self, z):\n",
        "    h = torch.relu(self.fc4(z)) #FILL IN STUDENT\n",
        "    h = torch.relu(self.fc5(h)) #FILL IN STUDENT\n",
        "    return torch.sigmoid(self.fc6(h)).view(-1,n_channels, n_rows,n_cols) #FILL IN STUDENT\n",
        "\n",
        "  def sampling(self, mu, log_var):\n",
        "    # this function samples a Gaussian distribution, with average (mu) and standard deviation specified (using log_var)\n",
        "    std = torch.exp(0.5*log_var)\n",
        "    eps = torch.randn_like(std)\n",
        "    return eps.mul(std).add_(mu) # return z sample\n",
        "\n",
        "  def forward(self, x):\n",
        "    z_mu, z_log_var = self.encoder(x.view(-1, self.n_pixels)) #FILL IN STUDENT\n",
        "    z = self.sampling(z_mu, z_log_var) #FILL IN STUDENT\n",
        "    return self.decoder(z),z_mu, z_log_var\n",
        "\n",
        "  def loss_function(self,x, y, mu, log_var):\n",
        "    reconstruction_error = F.binary_cross_entropy(y.view(-1, self.n_pixels), x.view(-1, self.n_pixels), reduction='sum') #FILL IN STUDENT\n",
        "    #reconstruction_error =  torch.nn.MSELoss(reduction='sum')(y.view(-1,self.n_pixels),x.view(-1,self.n_pixels))#F.binary_cross_entropy(y.view(-1,self.n_pixels),x.view(-1,self.n_pixels), reduction='sum')\n",
        "    \n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) #FILL IN STUDENT\n",
        "\n",
        "\n",
        "    return reconstruction_error + KLD #FILL IN STUDENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk_9fDIphlsi"
      },
      "source": [
        "Now, create the model (similarly as above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "pVlpC2R3htyU"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "vae_dim_1 = ae_dim_1\n",
        "vae_dim_2 = ae_dim_2\n",
        "vae_model = VAE(x_dim=n_pixels, h_dim1= vae_dim_1, h_dim2=vae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
        "vae_optimizer = optim.Adam(vae_model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYKLF_oMh5HO"
      },
      "source": [
        "Finally, train the model. First modify the training function to the case of the vae."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "z6DjKTWmmssb"
      },
      "outputs": [],
      "source": [
        "def train_vae(vae_model,data_train_loader,epoch):\n",
        "  train_loss = 0\n",
        "  for batch_idx, (data, _) in enumerate(data_train_loader):\n",
        "    vae_optimizer.zero_grad()\n",
        "\n",
        "    y, z_mu, z_log_var = vae_model(data) #FILL IN STUDENT\n",
        "    loss_vae = vae_model.loss_function(data, y, z_mu, z_log_var) #FILL IN STUDENT\n",
        "    loss_vae.backward()\n",
        "    train_loss += loss_vae.item()\n",
        "    vae_optimizer.step() \n",
        "\t\t\n",
        "    if batch_idx % 100 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "      epoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
        "      100. * batch_idx / len(data_train_loader), loss_vae.item() / len(data)))\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "L9JUUs6Kh8HB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/1000 (0%)]\tLoss: 140.012634\n",
            "====> Epoch: 0 Average loss: 111.0660\n",
            "Train Epoch: 1 [0/1000 (0%)]\tLoss: 144.390869\n",
            "====> Epoch: 1 Average loss: 111.6866\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLoss: 148.084961\n",
            "====> Epoch: 2 Average loss: 110.8087\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLoss: 143.028412\n",
            "====> Epoch: 3 Average loss: 110.2525\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLoss: 146.614166\n",
            "====> Epoch: 4 Average loss: 111.2295\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLoss: 143.869553\n",
            "====> Epoch: 5 Average loss: 110.0758\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLoss: 146.019562\n",
            "====> Epoch: 6 Average loss: 109.7767\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLoss: 145.567963\n",
            "====> Epoch: 7 Average loss: 110.1737\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLoss: 139.882523\n",
            "====> Epoch: 8 Average loss: 109.1992\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLoss: 147.182663\n",
            "====> Epoch: 9 Average loss: 109.4807\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLoss: 143.154190\n",
            "====> Epoch: 10 Average loss: 108.9373\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLoss: 142.276962\n",
            "====> Epoch: 11 Average loss: 108.3552\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLoss: 145.124054\n",
            "====> Epoch: 12 Average loss: 109.6114\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLoss: 139.790283\n",
            "====> Epoch: 13 Average loss: 108.9615\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLoss: 139.589645\n",
            "====> Epoch: 14 Average loss: 107.6427\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLoss: 138.747253\n",
            "====> Epoch: 15 Average loss: 107.4788\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLoss: 138.042770\n",
            "====> Epoch: 16 Average loss: 106.3586\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLoss: 135.004745\n",
            "====> Epoch: 17 Average loss: 106.9016\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLoss: 135.997620\n",
            "====> Epoch: 18 Average loss: 106.4538\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLoss: 140.727997\n",
            "====> Epoch: 19 Average loss: 106.1030\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLoss: 135.970123\n",
            "====> Epoch: 20 Average loss: 105.9716\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLoss: 136.289505\n",
            "====> Epoch: 21 Average loss: 106.0061\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLoss: 136.468750\n",
            "====> Epoch: 22 Average loss: 105.8507\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLoss: 138.358002\n",
            "====> Epoch: 23 Average loss: 104.9569\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLoss: 137.757751\n",
            "====> Epoch: 24 Average loss: 105.4318\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLoss: 135.137573\n",
            "====> Epoch: 25 Average loss: 104.8008\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLoss: 137.448135\n",
            "====> Epoch: 26 Average loss: 104.4909\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLoss: 135.787598\n",
            "====> Epoch: 27 Average loss: 104.1217\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLoss: 135.700317\n",
            "====> Epoch: 28 Average loss: 104.7001\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLoss: 133.000397\n",
            "====> Epoch: 29 Average loss: 104.1102\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLoss: 135.091156\n",
            "====> Epoch: 30 Average loss: 103.1050\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLoss: 136.229691\n",
            "====> Epoch: 31 Average loss: 103.1179\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLoss: 133.768616\n",
            "====> Epoch: 32 Average loss: 102.5869\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLoss: 133.621292\n",
            "====> Epoch: 33 Average loss: 103.3637\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLoss: 131.752213\n",
            "====> Epoch: 34 Average loss: 102.9166\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLoss: 135.890030\n",
            "====> Epoch: 35 Average loss: 102.4499\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLoss: 136.952881\n",
            "====> Epoch: 36 Average loss: 102.5466\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLoss: 134.735291\n",
            "====> Epoch: 37 Average loss: 102.3663\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLoss: 133.230591\n",
            "====> Epoch: 38 Average loss: 101.3403\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLoss: 133.518036\n",
            "====> Epoch: 39 Average loss: 101.6045\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLoss: 129.459290\n",
            "====> Epoch: 40 Average loss: 101.2554\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLoss: 132.808777\n",
            "====> Epoch: 41 Average loss: 101.1172\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLoss: 132.805649\n",
            "====> Epoch: 42 Average loss: 101.6953\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLoss: 132.938766\n",
            "====> Epoch: 43 Average loss: 99.9625\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLoss: 133.575043\n",
            "====> Epoch: 44 Average loss: 100.6360\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLoss: 127.312744\n",
            "====> Epoch: 45 Average loss: 99.9719\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLoss: 134.700424\n",
            "====> Epoch: 46 Average loss: 100.2150\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLoss: 132.441895\n",
            "====> Epoch: 47 Average loss: 100.4092\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLoss: 131.116455\n",
            "====> Epoch: 48 Average loss: 100.0867\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLoss: 129.853027\n",
            "====> Epoch: 49 Average loss: 99.6995\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLoss: 125.199387\n",
            "====> Epoch: 50 Average loss: 99.1592\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLoss: 129.829300\n",
            "====> Epoch: 51 Average loss: 99.4617\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLoss: 128.407990\n",
            "====> Epoch: 52 Average loss: 99.4746\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLoss: 125.337631\n",
            "====> Epoch: 53 Average loss: 99.3165\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLoss: 130.458664\n",
            "====> Epoch: 54 Average loss: 98.4897\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLoss: 130.601212\n",
            "====> Epoch: 55 Average loss: 98.4978\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLoss: 128.839417\n",
            "====> Epoch: 56 Average loss: 97.4824\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLoss: 129.818268\n",
            "====> Epoch: 57 Average loss: 98.5517\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLoss: 128.173401\n",
            "====> Epoch: 58 Average loss: 97.5050\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLoss: 126.576309\n",
            "====> Epoch: 59 Average loss: 97.0056\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLoss: 126.664642\n",
            "====> Epoch: 60 Average loss: 98.1435\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLoss: 123.568237\n",
            "====> Epoch: 61 Average loss: 96.0366\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLoss: 125.563225\n",
            "====> Epoch: 62 Average loss: 97.0061\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLoss: 123.765549\n",
            "====> Epoch: 63 Average loss: 97.3530\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLoss: 129.680603\n",
            "====> Epoch: 64 Average loss: 97.1413\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLoss: 124.281403\n",
            "====> Epoch: 65 Average loss: 96.2725\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLoss: 122.704117\n",
            "====> Epoch: 66 Average loss: 96.0548\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLoss: 127.805603\n",
            "====> Epoch: 67 Average loss: 95.5855\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLoss: 125.562363\n",
            "====> Epoch: 68 Average loss: 96.7368\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLoss: 122.506729\n",
            "====> Epoch: 69 Average loss: 95.8464\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLoss: 120.083351\n",
            "====> Epoch: 70 Average loss: 95.2369\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLoss: 124.117279\n",
            "====> Epoch: 71 Average loss: 95.1812\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLoss: 121.056946\n",
            "====> Epoch: 72 Average loss: 95.6335\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLoss: 122.675774\n",
            "====> Epoch: 73 Average loss: 95.3641\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLoss: 123.075027\n",
            "====> Epoch: 74 Average loss: 95.6983\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLoss: 123.191437\n",
            "====> Epoch: 75 Average loss: 94.4510\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLoss: 124.014175\n",
            "====> Epoch: 76 Average loss: 95.5432\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLoss: 121.346222\n",
            "====> Epoch: 77 Average loss: 94.9380\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLoss: 122.033691\n",
            "====> Epoch: 78 Average loss: 94.2378\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLoss: 123.877922\n",
            "====> Epoch: 79 Average loss: 95.2795\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLoss: 125.341141\n",
            "====> Epoch: 80 Average loss: 94.5718\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLoss: 121.831795\n",
            "====> Epoch: 81 Average loss: 93.9039\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLoss: 119.292480\n",
            "====> Epoch: 82 Average loss: 93.6335\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLoss: 120.278069\n",
            "====> Epoch: 83 Average loss: 93.7193\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLoss: 120.596169\n",
            "====> Epoch: 84 Average loss: 93.5154\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLoss: 120.392365\n",
            "====> Epoch: 85 Average loss: 93.1814\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLoss: 121.798294\n",
            "====> Epoch: 86 Average loss: 93.4575\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLoss: 122.497078\n",
            "====> Epoch: 87 Average loss: 93.4498\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLoss: 119.162369\n",
            "====> Epoch: 88 Average loss: 92.5160\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLoss: 119.602974\n",
            "====> Epoch: 89 Average loss: 93.1329\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLoss: 120.177109\n",
            "====> Epoch: 90 Average loss: 92.7575\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLoss: 121.337585\n",
            "====> Epoch: 91 Average loss: 93.3141\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLoss: 121.933556\n",
            "====> Epoch: 92 Average loss: 93.5108\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLoss: 118.863113\n",
            "====> Epoch: 93 Average loss: 93.1601\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLoss: 117.509888\n",
            "====> Epoch: 94 Average loss: 92.3264\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLoss: 117.143600\n",
            "====> Epoch: 95 Average loss: 92.6480\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLoss: 118.383080\n",
            "====> Epoch: 96 Average loss: 93.2690\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLoss: 120.946274\n",
            "====> Epoch: 97 Average loss: 91.1828\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLoss: 121.099594\n",
            "====> Epoch: 98 Average loss: 93.0776\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLoss: 121.744522\n",
            "====> Epoch: 99 Average loss: 91.7543\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLoss: 118.781189\n",
            "====> Epoch: 100 Average loss: 90.8140\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLoss: 119.345535\n",
            "====> Epoch: 101 Average loss: 90.8832\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLoss: 117.935608\n",
            "====> Epoch: 102 Average loss: 91.6663\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLoss: 117.216682\n",
            "====> Epoch: 103 Average loss: 90.7326\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLoss: 117.886810\n",
            "====> Epoch: 104 Average loss: 91.9934\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLoss: 118.516479\n",
            "====> Epoch: 105 Average loss: 91.4823\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLoss: 119.416534\n",
            "====> Epoch: 106 Average loss: 90.8992\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLoss: 118.524506\n",
            "====> Epoch: 107 Average loss: 90.9707\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLoss: 119.378555\n",
            "====> Epoch: 108 Average loss: 91.1388\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLoss: 118.131371\n",
            "====> Epoch: 109 Average loss: 90.7910\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLoss: 121.424637\n",
            "====> Epoch: 110 Average loss: 90.9943\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLoss: 117.001305\n",
            "====> Epoch: 111 Average loss: 89.6166\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLoss: 116.302132\n",
            "====> Epoch: 112 Average loss: 89.6047\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLoss: 118.815384\n",
            "====> Epoch: 113 Average loss: 89.2109\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLoss: 118.825684\n",
            "====> Epoch: 114 Average loss: 90.4099\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLoss: 115.265610\n",
            "====> Epoch: 115 Average loss: 89.2904\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLoss: 117.240028\n",
            "====> Epoch: 116 Average loss: 89.4962\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLoss: 116.126022\n",
            "====> Epoch: 117 Average loss: 89.5541\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLoss: 117.539719\n",
            "====> Epoch: 118 Average loss: 89.0019\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLoss: 117.799675\n",
            "====> Epoch: 119 Average loss: 89.6170\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLoss: 119.264137\n",
            "====> Epoch: 120 Average loss: 89.8715\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLoss: 117.853592\n",
            "====> Epoch: 121 Average loss: 88.6378\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLoss: 118.940819\n",
            "====> Epoch: 122 Average loss: 89.7620\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLoss: 117.207512\n",
            "====> Epoch: 123 Average loss: 89.1402\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLoss: 116.724396\n",
            "====> Epoch: 124 Average loss: 89.8445\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLoss: 116.097900\n",
            "====> Epoch: 125 Average loss: 88.9672\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLoss: 117.504448\n",
            "====> Epoch: 126 Average loss: 89.0579\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLoss: 113.490486\n",
            "====> Epoch: 127 Average loss: 88.0647\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLoss: 116.364159\n",
            "====> Epoch: 128 Average loss: 88.0691\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLoss: 116.069229\n",
            "====> Epoch: 129 Average loss: 87.9517\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLoss: 115.889557\n",
            "====> Epoch: 130 Average loss: 88.5035\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLoss: 111.280899\n",
            "====> Epoch: 131 Average loss: 87.7149\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLoss: 112.013733\n",
            "====> Epoch: 132 Average loss: 88.1002\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLoss: 113.235016\n",
            "====> Epoch: 133 Average loss: 88.4325\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLoss: 111.472275\n",
            "====> Epoch: 134 Average loss: 87.4671\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLoss: 110.759460\n",
            "====> Epoch: 135 Average loss: 87.7392\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLoss: 114.216309\n",
            "====> Epoch: 136 Average loss: 87.9635\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLoss: 113.510231\n",
            "====> Epoch: 137 Average loss: 88.0788\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLoss: 114.057159\n",
            "====> Epoch: 138 Average loss: 87.6590\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLoss: 113.595314\n",
            "====> Epoch: 139 Average loss: 86.9926\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLoss: 113.779190\n",
            "====> Epoch: 140 Average loss: 87.0521\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLoss: 114.367874\n",
            "====> Epoch: 141 Average loss: 87.3596\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLoss: 113.076752\n",
            "====> Epoch: 142 Average loss: 87.2072\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLoss: 113.548935\n",
            "====> Epoch: 143 Average loss: 86.3972\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLoss: 111.050064\n",
            "====> Epoch: 144 Average loss: 86.5946\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLoss: 110.267471\n",
            "====> Epoch: 145 Average loss: 87.0503\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLoss: 112.105179\n",
            "====> Epoch: 146 Average loss: 86.6905\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLoss: 113.440247\n",
            "====> Epoch: 147 Average loss: 86.5907\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLoss: 113.879501\n",
            "====> Epoch: 148 Average loss: 86.6446\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLoss: 113.030380\n",
            "====> Epoch: 149 Average loss: 86.0260\n",
            "Train Epoch: 150 [0/1000 (0%)]\tLoss: 110.055473\n",
            "====> Epoch: 150 Average loss: 85.1534\n",
            "Train Epoch: 151 [0/1000 (0%)]\tLoss: 110.434601\n",
            "====> Epoch: 151 Average loss: 85.9763\n",
            "Train Epoch: 152 [0/1000 (0%)]\tLoss: 116.227325\n",
            "====> Epoch: 152 Average loss: 86.4033\n",
            "Train Epoch: 153 [0/1000 (0%)]\tLoss: 110.983826\n",
            "====> Epoch: 153 Average loss: 86.0817\n",
            "Train Epoch: 154 [0/1000 (0%)]\tLoss: 110.511497\n",
            "====> Epoch: 154 Average loss: 85.9450\n",
            "Train Epoch: 155 [0/1000 (0%)]\tLoss: 110.050690\n",
            "====> Epoch: 155 Average loss: 85.9458\n",
            "Train Epoch: 156 [0/1000 (0%)]\tLoss: 109.744171\n",
            "====> Epoch: 156 Average loss: 85.2430\n",
            "Train Epoch: 157 [0/1000 (0%)]\tLoss: 112.376968\n",
            "====> Epoch: 157 Average loss: 85.7139\n",
            "Train Epoch: 158 [0/1000 (0%)]\tLoss: 110.942139\n",
            "====> Epoch: 158 Average loss: 85.5860\n",
            "Train Epoch: 159 [0/1000 (0%)]\tLoss: 112.192375\n",
            "====> Epoch: 159 Average loss: 85.5110\n",
            "Train Epoch: 160 [0/1000 (0%)]\tLoss: 110.494064\n",
            "====> Epoch: 160 Average loss: 86.1896\n",
            "Train Epoch: 161 [0/1000 (0%)]\tLoss: 109.489639\n",
            "====> Epoch: 161 Average loss: 85.5324\n",
            "Train Epoch: 162 [0/1000 (0%)]\tLoss: 111.883835\n",
            "====> Epoch: 162 Average loss: 85.0332\n",
            "Train Epoch: 163 [0/1000 (0%)]\tLoss: 109.791733\n",
            "====> Epoch: 163 Average loss: 85.0177\n",
            "Train Epoch: 164 [0/1000 (0%)]\tLoss: 109.578156\n",
            "====> Epoch: 164 Average loss: 84.9500\n",
            "Train Epoch: 165 [0/1000 (0%)]\tLoss: 110.380180\n",
            "====> Epoch: 165 Average loss: 84.3630\n",
            "Train Epoch: 166 [0/1000 (0%)]\tLoss: 112.241219\n",
            "====> Epoch: 166 Average loss: 85.1936\n",
            "Train Epoch: 167 [0/1000 (0%)]\tLoss: 111.184311\n",
            "====> Epoch: 167 Average loss: 84.3950\n",
            "Train Epoch: 168 [0/1000 (0%)]\tLoss: 112.248276\n",
            "====> Epoch: 168 Average loss: 85.6314\n",
            "Train Epoch: 169 [0/1000 (0%)]\tLoss: 110.679176\n",
            "====> Epoch: 169 Average loss: 83.7910\n",
            "Train Epoch: 170 [0/1000 (0%)]\tLoss: 110.663132\n",
            "====> Epoch: 170 Average loss: 85.2396\n",
            "Train Epoch: 171 [0/1000 (0%)]\tLoss: 111.803192\n",
            "====> Epoch: 171 Average loss: 83.7751\n",
            "Train Epoch: 172 [0/1000 (0%)]\tLoss: 107.149544\n",
            "====> Epoch: 172 Average loss: 84.3463\n",
            "Train Epoch: 173 [0/1000 (0%)]\tLoss: 108.554382\n",
            "====> Epoch: 173 Average loss: 83.8702\n",
            "Train Epoch: 174 [0/1000 (0%)]\tLoss: 109.816116\n",
            "====> Epoch: 174 Average loss: 83.9607\n",
            "Train Epoch: 175 [0/1000 (0%)]\tLoss: 107.225601\n",
            "====> Epoch: 175 Average loss: 84.4941\n",
            "Train Epoch: 176 [0/1000 (0%)]\tLoss: 108.758179\n",
            "====> Epoch: 176 Average loss: 84.0222\n",
            "Train Epoch: 177 [0/1000 (0%)]\tLoss: 110.199730\n",
            "====> Epoch: 177 Average loss: 84.1582\n",
            "Train Epoch: 178 [0/1000 (0%)]\tLoss: 105.936470\n",
            "====> Epoch: 178 Average loss: 83.6310\n",
            "Train Epoch: 179 [0/1000 (0%)]\tLoss: 108.239944\n",
            "====> Epoch: 179 Average loss: 84.1840\n",
            "Train Epoch: 180 [0/1000 (0%)]\tLoss: 111.424469\n",
            "====> Epoch: 180 Average loss: 84.6417\n",
            "Train Epoch: 181 [0/1000 (0%)]\tLoss: 110.024811\n",
            "====> Epoch: 181 Average loss: 83.5193\n",
            "Train Epoch: 182 [0/1000 (0%)]\tLoss: 108.169075\n",
            "====> Epoch: 182 Average loss: 83.3726\n",
            "Train Epoch: 183 [0/1000 (0%)]\tLoss: 109.103477\n",
            "====> Epoch: 183 Average loss: 83.4828\n",
            "Train Epoch: 184 [0/1000 (0%)]\tLoss: 110.412796\n",
            "====> Epoch: 184 Average loss: 84.3712\n",
            "Train Epoch: 185 [0/1000 (0%)]\tLoss: 108.629036\n",
            "====> Epoch: 185 Average loss: 83.3384\n",
            "Train Epoch: 186 [0/1000 (0%)]\tLoss: 107.122879\n",
            "====> Epoch: 186 Average loss: 83.0715\n",
            "Train Epoch: 187 [0/1000 (0%)]\tLoss: 111.195114\n",
            "====> Epoch: 187 Average loss: 82.9210\n",
            "Train Epoch: 188 [0/1000 (0%)]\tLoss: 106.182632\n",
            "====> Epoch: 188 Average loss: 82.9846\n",
            "Train Epoch: 189 [0/1000 (0%)]\tLoss: 108.213158\n",
            "====> Epoch: 189 Average loss: 82.8452\n",
            "Train Epoch: 190 [0/1000 (0%)]\tLoss: 108.875740\n",
            "====> Epoch: 190 Average loss: 83.1168\n",
            "Train Epoch: 191 [0/1000 (0%)]\tLoss: 108.873466\n",
            "====> Epoch: 191 Average loss: 82.4672\n",
            "Train Epoch: 192 [0/1000 (0%)]\tLoss: 108.334869\n",
            "====> Epoch: 192 Average loss: 82.6457\n",
            "Train Epoch: 193 [0/1000 (0%)]\tLoss: 105.939415\n",
            "====> Epoch: 193 Average loss: 82.0855\n",
            "Train Epoch: 194 [0/1000 (0%)]\tLoss: 107.599442\n",
            "====> Epoch: 194 Average loss: 82.0788\n",
            "Train Epoch: 195 [0/1000 (0%)]\tLoss: 109.037560\n",
            "====> Epoch: 195 Average loss: 81.7124\n",
            "Train Epoch: 196 [0/1000 (0%)]\tLoss: 107.757538\n",
            "====> Epoch: 196 Average loss: 81.8831\n",
            "Train Epoch: 197 [0/1000 (0%)]\tLoss: 106.773514\n",
            "====> Epoch: 197 Average loss: 82.4974\n",
            "Train Epoch: 198 [0/1000 (0%)]\tLoss: 107.258774\n",
            "====> Epoch: 198 Average loss: 81.8537\n",
            "Train Epoch: 199 [0/1000 (0%)]\tLoss: 104.709557\n",
            "====> Epoch: 199 Average loss: 81.8622\n",
            "Train Epoch: 200 [0/1000 (0%)]\tLoss: 104.679306\n",
            "====> Epoch: 200 Average loss: 82.0268\n",
            "Train Epoch: 201 [0/1000 (0%)]\tLoss: 106.963051\n",
            "====> Epoch: 201 Average loss: 81.7209\n",
            "Train Epoch: 202 [0/1000 (0%)]\tLoss: 107.003922\n",
            "====> Epoch: 202 Average loss: 82.3144\n",
            "Train Epoch: 203 [0/1000 (0%)]\tLoss: 105.722466\n",
            "====> Epoch: 203 Average loss: 81.6093\n",
            "Train Epoch: 204 [0/1000 (0%)]\tLoss: 107.317673\n",
            "====> Epoch: 204 Average loss: 82.3295\n",
            "Train Epoch: 205 [0/1000 (0%)]\tLoss: 107.456390\n",
            "====> Epoch: 205 Average loss: 81.5219\n",
            "Train Epoch: 206 [0/1000 (0%)]\tLoss: 106.774750\n",
            "====> Epoch: 206 Average loss: 81.9857\n",
            "Train Epoch: 207 [0/1000 (0%)]\tLoss: 105.124687\n",
            "====> Epoch: 207 Average loss: 81.4200\n",
            "Train Epoch: 208 [0/1000 (0%)]\tLoss: 107.308578\n",
            "====> Epoch: 208 Average loss: 81.3010\n",
            "Train Epoch: 209 [0/1000 (0%)]\tLoss: 105.777214\n",
            "====> Epoch: 209 Average loss: 81.3717\n",
            "Train Epoch: 210 [0/1000 (0%)]\tLoss: 101.756790\n",
            "====> Epoch: 210 Average loss: 81.1164\n",
            "Train Epoch: 211 [0/1000 (0%)]\tLoss: 105.292793\n",
            "====> Epoch: 211 Average loss: 81.6554\n",
            "Train Epoch: 212 [0/1000 (0%)]\tLoss: 103.402145\n",
            "====> Epoch: 212 Average loss: 81.0713\n",
            "Train Epoch: 213 [0/1000 (0%)]\tLoss: 106.613670\n",
            "====> Epoch: 213 Average loss: 81.8708\n",
            "Train Epoch: 214 [0/1000 (0%)]\tLoss: 102.346573\n",
            "====> Epoch: 214 Average loss: 80.2902\n",
            "Train Epoch: 215 [0/1000 (0%)]\tLoss: 107.841568\n",
            "====> Epoch: 215 Average loss: 81.0315\n",
            "Train Epoch: 216 [0/1000 (0%)]\tLoss: 103.192940\n",
            "====> Epoch: 216 Average loss: 81.2303\n",
            "Train Epoch: 217 [0/1000 (0%)]\tLoss: 104.598412\n",
            "====> Epoch: 217 Average loss: 81.3258\n",
            "Train Epoch: 218 [0/1000 (0%)]\tLoss: 103.613419\n",
            "====> Epoch: 218 Average loss: 80.2463\n",
            "Train Epoch: 219 [0/1000 (0%)]\tLoss: 101.089577\n",
            "====> Epoch: 219 Average loss: 79.8904\n",
            "Train Epoch: 220 [0/1000 (0%)]\tLoss: 104.150391\n",
            "====> Epoch: 220 Average loss: 81.0720\n",
            "Train Epoch: 221 [0/1000 (0%)]\tLoss: 105.549835\n",
            "====> Epoch: 221 Average loss: 81.1532\n",
            "Train Epoch: 222 [0/1000 (0%)]\tLoss: 104.110550\n",
            "====> Epoch: 222 Average loss: 80.5389\n",
            "Train Epoch: 223 [0/1000 (0%)]\tLoss: 103.796593\n",
            "====> Epoch: 223 Average loss: 80.6057\n",
            "Train Epoch: 224 [0/1000 (0%)]\tLoss: 103.295174\n",
            "====> Epoch: 224 Average loss: 80.2167\n",
            "Train Epoch: 225 [0/1000 (0%)]\tLoss: 103.679192\n",
            "====> Epoch: 225 Average loss: 80.3415\n",
            "Train Epoch: 226 [0/1000 (0%)]\tLoss: 106.012962\n",
            "====> Epoch: 226 Average loss: 81.3675\n",
            "Train Epoch: 227 [0/1000 (0%)]\tLoss: 102.482285\n",
            "====> Epoch: 227 Average loss: 79.8728\n",
            "Train Epoch: 228 [0/1000 (0%)]\tLoss: 104.562157\n",
            "====> Epoch: 228 Average loss: 80.2089\n",
            "Train Epoch: 229 [0/1000 (0%)]\tLoss: 104.122269\n",
            "====> Epoch: 229 Average loss: 80.9021\n",
            "Train Epoch: 230 [0/1000 (0%)]\tLoss: 101.825760\n",
            "====> Epoch: 230 Average loss: 79.5107\n",
            "Train Epoch: 231 [0/1000 (0%)]\tLoss: 103.872337\n",
            "====> Epoch: 231 Average loss: 79.4035\n",
            "Train Epoch: 232 [0/1000 (0%)]\tLoss: 105.465897\n",
            "====> Epoch: 232 Average loss: 80.3733\n",
            "Train Epoch: 233 [0/1000 (0%)]\tLoss: 103.513824\n",
            "====> Epoch: 233 Average loss: 80.2370\n",
            "Train Epoch: 234 [0/1000 (0%)]\tLoss: 102.701248\n",
            "====> Epoch: 234 Average loss: 79.1765\n",
            "Train Epoch: 235 [0/1000 (0%)]\tLoss: 102.407608\n",
            "====> Epoch: 235 Average loss: 79.4106\n",
            "Train Epoch: 236 [0/1000 (0%)]\tLoss: 104.949646\n",
            "====> Epoch: 236 Average loss: 79.4877\n",
            "Train Epoch: 237 [0/1000 (0%)]\tLoss: 103.292778\n",
            "====> Epoch: 237 Average loss: 80.0906\n",
            "Train Epoch: 238 [0/1000 (0%)]\tLoss: 101.630959\n",
            "====> Epoch: 238 Average loss: 78.9993\n",
            "Train Epoch: 239 [0/1000 (0%)]\tLoss: 106.671509\n",
            "====> Epoch: 239 Average loss: 79.6416\n",
            "Train Epoch: 240 [0/1000 (0%)]\tLoss: 100.074799\n",
            "====> Epoch: 240 Average loss: 79.0610\n",
            "Train Epoch: 241 [0/1000 (0%)]\tLoss: 103.704872\n",
            "====> Epoch: 241 Average loss: 79.3860\n",
            "Train Epoch: 242 [0/1000 (0%)]\tLoss: 103.036240\n",
            "====> Epoch: 242 Average loss: 79.0457\n",
            "Train Epoch: 243 [0/1000 (0%)]\tLoss: 103.384239\n",
            "====> Epoch: 243 Average loss: 79.1859\n",
            "Train Epoch: 244 [0/1000 (0%)]\tLoss: 102.333267\n",
            "====> Epoch: 244 Average loss: 79.1715\n",
            "Train Epoch: 245 [0/1000 (0%)]\tLoss: 102.354012\n",
            "====> Epoch: 245 Average loss: 78.9539\n",
            "Train Epoch: 246 [0/1000 (0%)]\tLoss: 101.546890\n",
            "====> Epoch: 246 Average loss: 79.0398\n",
            "Train Epoch: 247 [0/1000 (0%)]\tLoss: 102.244698\n",
            "====> Epoch: 247 Average loss: 78.7151\n",
            "Train Epoch: 248 [0/1000 (0%)]\tLoss: 102.378769\n",
            "====> Epoch: 248 Average loss: 78.3119\n",
            "Train Epoch: 249 [0/1000 (0%)]\tLoss: 100.672058\n",
            "====> Epoch: 249 Average loss: 78.2474\n",
            "Train Epoch: 250 [0/1000 (0%)]\tLoss: 103.832977\n",
            "====> Epoch: 250 Average loss: 78.9476\n",
            "Train Epoch: 251 [0/1000 (0%)]\tLoss: 100.071121\n",
            "====> Epoch: 251 Average loss: 78.0017\n",
            "Train Epoch: 252 [0/1000 (0%)]\tLoss: 104.672585\n",
            "====> Epoch: 252 Average loss: 78.9252\n",
            "Train Epoch: 253 [0/1000 (0%)]\tLoss: 102.221214\n",
            "====> Epoch: 253 Average loss: 78.5341\n",
            "Train Epoch: 254 [0/1000 (0%)]\tLoss: 102.452232\n",
            "====> Epoch: 254 Average loss: 78.9714\n",
            "Train Epoch: 255 [0/1000 (0%)]\tLoss: 100.529266\n",
            "====> Epoch: 255 Average loss: 78.2582\n",
            "Train Epoch: 256 [0/1000 (0%)]\tLoss: 101.801529\n",
            "====> Epoch: 256 Average loss: 78.1888\n",
            "Train Epoch: 257 [0/1000 (0%)]\tLoss: 103.870186\n",
            "====> Epoch: 257 Average loss: 78.5567\n",
            "Train Epoch: 258 [0/1000 (0%)]\tLoss: 102.759781\n",
            "====> Epoch: 258 Average loss: 78.1048\n",
            "Train Epoch: 259 [0/1000 (0%)]\tLoss: 104.236244\n",
            "====> Epoch: 259 Average loss: 78.6179\n",
            "Train Epoch: 260 [0/1000 (0%)]\tLoss: 100.911942\n",
            "====> Epoch: 260 Average loss: 78.1147\n",
            "Train Epoch: 261 [0/1000 (0%)]\tLoss: 99.904633\n",
            "====> Epoch: 261 Average loss: 78.0648\n",
            "Train Epoch: 262 [0/1000 (0%)]\tLoss: 101.735367\n",
            "====> Epoch: 262 Average loss: 78.3065\n",
            "Train Epoch: 263 [0/1000 (0%)]\tLoss: 99.752823\n",
            "====> Epoch: 263 Average loss: 77.7452\n",
            "Train Epoch: 264 [0/1000 (0%)]\tLoss: 100.221970\n",
            "====> Epoch: 264 Average loss: 77.9719\n",
            "Train Epoch: 265 [0/1000 (0%)]\tLoss: 101.516953\n",
            "====> Epoch: 265 Average loss: 78.2672\n",
            "Train Epoch: 266 [0/1000 (0%)]\tLoss: 100.272079\n",
            "====> Epoch: 266 Average loss: 77.6714\n",
            "Train Epoch: 267 [0/1000 (0%)]\tLoss: 102.736725\n",
            "====> Epoch: 267 Average loss: 78.1054\n",
            "Train Epoch: 268 [0/1000 (0%)]\tLoss: 100.435638\n",
            "====> Epoch: 268 Average loss: 77.5064\n",
            "Train Epoch: 269 [0/1000 (0%)]\tLoss: 99.185104\n",
            "====> Epoch: 269 Average loss: 76.9835\n",
            "Train Epoch: 270 [0/1000 (0%)]\tLoss: 101.568169\n",
            "====> Epoch: 270 Average loss: 76.8316\n",
            "Train Epoch: 271 [0/1000 (0%)]\tLoss: 100.436340\n",
            "====> Epoch: 271 Average loss: 77.6069\n",
            "Train Epoch: 272 [0/1000 (0%)]\tLoss: 99.211243\n",
            "====> Epoch: 272 Average loss: 76.7168\n",
            "Train Epoch: 273 [0/1000 (0%)]\tLoss: 98.503815\n",
            "====> Epoch: 273 Average loss: 76.6329\n",
            "Train Epoch: 274 [0/1000 (0%)]\tLoss: 101.973320\n",
            "====> Epoch: 274 Average loss: 77.6123\n",
            "Train Epoch: 275 [0/1000 (0%)]\tLoss: 101.654747\n",
            "====> Epoch: 275 Average loss: 76.6839\n",
            "Train Epoch: 276 [0/1000 (0%)]\tLoss: 100.942543\n",
            "====> Epoch: 276 Average loss: 77.4710\n",
            "Train Epoch: 277 [0/1000 (0%)]\tLoss: 100.618675\n",
            "====> Epoch: 277 Average loss: 77.2746\n",
            "Train Epoch: 278 [0/1000 (0%)]\tLoss: 100.902435\n",
            "====> Epoch: 278 Average loss: 78.0493\n",
            "Train Epoch: 279 [0/1000 (0%)]\tLoss: 97.824661\n",
            "====> Epoch: 279 Average loss: 76.8286\n",
            "Train Epoch: 280 [0/1000 (0%)]\tLoss: 99.791924\n",
            "====> Epoch: 280 Average loss: 76.8146\n",
            "Train Epoch: 281 [0/1000 (0%)]\tLoss: 100.293503\n",
            "====> Epoch: 281 Average loss: 77.0275\n",
            "Train Epoch: 282 [0/1000 (0%)]\tLoss: 98.187775\n",
            "====> Epoch: 282 Average loss: 76.6545\n",
            "Train Epoch: 283 [0/1000 (0%)]\tLoss: 98.327141\n",
            "====> Epoch: 283 Average loss: 76.8672\n",
            "Train Epoch: 284 [0/1000 (0%)]\tLoss: 100.087776\n",
            "====> Epoch: 284 Average loss: 76.7799\n",
            "Train Epoch: 285 [0/1000 (0%)]\tLoss: 100.043663\n",
            "====> Epoch: 285 Average loss: 76.8070\n",
            "Train Epoch: 286 [0/1000 (0%)]\tLoss: 99.399033\n",
            "====> Epoch: 286 Average loss: 76.7852\n",
            "Train Epoch: 287 [0/1000 (0%)]\tLoss: 99.537132\n",
            "====> Epoch: 287 Average loss: 76.1334\n",
            "Train Epoch: 288 [0/1000 (0%)]\tLoss: 98.288727\n",
            "====> Epoch: 288 Average loss: 76.4765\n",
            "Train Epoch: 289 [0/1000 (0%)]\tLoss: 97.694275\n",
            "====> Epoch: 289 Average loss: 75.9638\n",
            "Train Epoch: 290 [0/1000 (0%)]\tLoss: 98.226395\n",
            "====> Epoch: 290 Average loss: 76.0032\n",
            "Train Epoch: 291 [0/1000 (0%)]\tLoss: 100.702118\n",
            "====> Epoch: 291 Average loss: 76.1914\n",
            "Train Epoch: 292 [0/1000 (0%)]\tLoss: 99.247360\n",
            "====> Epoch: 292 Average loss: 76.5355\n",
            "Train Epoch: 293 [0/1000 (0%)]\tLoss: 99.825836\n",
            "====> Epoch: 293 Average loss: 76.5634\n",
            "Train Epoch: 294 [0/1000 (0%)]\tLoss: 99.177727\n",
            "====> Epoch: 294 Average loss: 76.8086\n",
            "Train Epoch: 295 [0/1000 (0%)]\tLoss: 101.154587\n",
            "====> Epoch: 295 Average loss: 76.5263\n",
            "Train Epoch: 296 [0/1000 (0%)]\tLoss: 99.503586\n",
            "====> Epoch: 296 Average loss: 75.7981\n",
            "Train Epoch: 297 [0/1000 (0%)]\tLoss: 100.255608\n",
            "====> Epoch: 297 Average loss: 75.7028\n",
            "Train Epoch: 298 [0/1000 (0%)]\tLoss: 98.047646\n",
            "====> Epoch: 298 Average loss: 75.8103\n",
            "Train Epoch: 299 [0/1000 (0%)]\tLoss: 98.168961\n",
            "====> Epoch: 299 Average loss: 75.7057\n"
          ]
        }
      ],
      "source": [
        "# now train the model\n",
        "for epoch in range(0, n_epochs):\n",
        "  train_vae(vae_model,mnist_train_loader,epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTfRje_AkKDr"
      },
      "source": [
        "Now, generate some images with the VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "41tXdNsFkKk5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_images_vae(vae_model,n_images = 5):\n",
        "  epsilon = torch.randn(n_images,1,vae_model.z_dim)\n",
        "  imgs_generated = vae_model.decoder(epsilon)\n",
        "  return(imgs_generated)\n",
        "\n",
        "imgs_generated = generate_images_vae(vae_model,n_images=5)\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnvKoynzaFN"
      },
      "source": [
        "What do you think of the results ? You can try and change the latent space size, or use convolutional layers instead to improve the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RBM:\n",
        "\n",
        "  def __init__(self, p, q):\n",
        "    self.a = np.zeros(p)\n",
        "    self.b = np.zeros(q)\n",
        "    self.W = np.random.normal(loc=0.0, scale=0.1, size=(p, q))\n",
        "\n",
        "  def entree_sortie_RBM(self, X):\n",
        "    return 1/(1 + np.exp(-(X@self.W + self.b)))\n",
        "\n",
        "  def sortie_entree_RBM(self, H):\n",
        "    return 1/( 1 + np.exp(-(H@self.W.T + self.a)))\n",
        "\n",
        "  def train_RBM(self, X, epsilon, batch_size, nb_epochs):\n",
        "    p = self.a.shape[0]\n",
        "    q = self.b.shape[0]\n",
        "    n = np.size(X, 0)\n",
        "    shuffle_index = np.arange(n)\n",
        "    error = []\n",
        "    for epoch in range(0, nb_epochs):\n",
        "      np.random.shuffle(shuffle_index)\n",
        "      x = X[shuffle_index]\n",
        "      for i in range(0, n, batch_size):\n",
        "        X_batch = x[i: min(i+batch_size, n-1), :]\n",
        "        tb = np.size(X_batch, 0)\n",
        "\n",
        "        v_0 = X_batch\n",
        "        p_h_v_0 = self.entree_sortie_RBM(v_0)\n",
        "        h_0 = (np.random.rand(tb, q) < p_h_v_0) * 1\n",
        "\n",
        "        p_v_h_0 = self.sortie_entree_RBM(h_0)\n",
        "        v_1 = (np.random.rand(tb, p) < p_v_h_0) * 1\n",
        "        p_h_v_1 = self.entree_sortie_RBM(v_1)\n",
        "\n",
        "        grad_a = np.sum(v_0-v_1, axis=0)\n",
        "        grad_b = np.sum(p_h_v_0-p_h_v_1, axis=0)\n",
        "        grad_W = v_0.T@p_h_v_0 - v_1.T@p_h_v_1\n",
        "\n",
        "        self.a += (epsilon / tb) * grad_a\n",
        "        self.b += (epsilon / tb) * grad_b\n",
        "        self.W += (epsilon / tb) * grad_W\n",
        "\n",
        "      H = self.entree_sortie_RBM(X)\n",
        "      X_rec = self.sortie_entree_RBM(H)\n",
        "      err = np.sum((X - X_rec) ** 2) / n\n",
        "      error.append(err)\n",
        "      print(\"l'erreur  l'epoch \" + str(epoch) + \" est :\" + str(err))\n",
        "    return error\n",
        "\n",
        "  def generer_image_RBM(self, nb_data, nb_gibbs):\n",
        "    p = self.a.shape[0]\n",
        "    q = self.b.shape[0]\n",
        "    fig, axs = plt.subplots(1, nb_data, figsize=(15, 5), constrained_layout=True)\n",
        "    for i in range(0, nb_data):\n",
        "        v = (np.random.rand(p) < 0.5) * 1\n",
        "        for iter_gibbs in range(0, nb_gibbs):\n",
        "            h = (np.random.rand(q) < self.entree_sortie_RBM(v)) * 1\n",
        "            v = (np.random.rand(p) < self.sortie_entree_RBM(h)) * 1\n",
        "        v = v.reshape(28, 28)\n",
        "        axs[i].imshow(v, cmap='gray')\n",
        "        axs[i].axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "from idx2numpy import convert_from_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = convert_from_file(\"mnist_data/raw/train-images-idx3-ubyte\")\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_train = np.array(X_train > 0.5, dtype=np.int8)\n",
        "\n",
        "X_test = convert_from_file(\"mnist_data/raw/t10k-images-idx3-ubyte\")\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "X_test = np.array(X_test > 0.5, dtype=np.int8)\n",
        "\n",
        "label_training = convert_from_file(\"mnist_data/raw/train-labels-idx1-ubyte\")\n",
        "label_training = np.eye(10)[label_training]\n",
        "\n",
        "label_test = convert_from_file(\"mnist_data/raw/t10k-labels-idx1-ubyte\")\n",
        "label_test = np.eye(10)[label_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def get_random_samples(X, k=1000):\n",
        "  n = np.size(X, 0)\n",
        "  indices = np.arange(n)\n",
        "  np.random.shuffle(indices)\n",
        "  sampled_indices = indices[:k]\n",
        "  return X[sampled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = get_random_samples(X_train, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "l'erreur  l'epoch 0 est :54.871593493588975\n",
            "l'erreur  l'epoch 1 est :43.95764591920783\n",
            "l'erreur  l'epoch 2 est :37.04369413915457\n",
            "l'erreur  l'epoch 3 est :33.08668035988388\n",
            "l'erreur  l'epoch 4 est :31.387546889191857\n",
            "l'erreur  l'epoch 5 est :28.706311611450932\n",
            "l'erreur  l'epoch 6 est :26.322831785700032\n",
            "l'erreur  l'epoch 7 est :26.185680415100098\n",
            "l'erreur  l'epoch 8 est :24.155622410400188\n",
            "l'erreur  l'epoch 9 est :23.285276003241474\n",
            "l'erreur  l'epoch 10 est :21.782543071525396\n",
            "l'erreur  l'epoch 11 est :21.362774690549895\n",
            "l'erreur  l'epoch 12 est :20.740830178915534\n",
            "l'erreur  l'epoch 13 est :20.303864180750086\n",
            "l'erreur  l'epoch 14 est :19.25229505444621\n",
            "l'erreur  l'epoch 15 est :18.6282201041795\n",
            "l'erreur  l'epoch 16 est :18.532131825858226\n",
            "l'erreur  l'epoch 17 est :17.886428152100518\n",
            "l'erreur  l'epoch 18 est :17.302103273092296\n",
            "l'erreur  l'epoch 19 est :16.94858692804165\n",
            "l'erreur  l'epoch 20 est :16.218366938532082\n",
            "l'erreur  l'epoch 21 est :15.607716202542688\n",
            "l'erreur  l'epoch 22 est :15.368095423416083\n",
            "l'erreur  l'epoch 23 est :15.572236322693255\n",
            "l'erreur  l'epoch 24 est :15.009715219653991\n",
            "l'erreur  l'epoch 25 est :14.268090409936423\n",
            "l'erreur  l'epoch 26 est :13.868639772315888\n",
            "l'erreur  l'epoch 27 est :13.555508218017957\n",
            "l'erreur  l'epoch 28 est :13.074858046604572\n",
            "l'erreur  l'epoch 29 est :13.118950572348183\n",
            "l'erreur  l'epoch 30 est :13.316151703725673\n",
            "l'erreur  l'epoch 31 est :12.901562865092208\n",
            "l'erreur  l'epoch 32 est :12.018325431961978\n",
            "l'erreur  l'epoch 33 est :11.887840516018919\n",
            "l'erreur  l'epoch 34 est :11.720886831105522\n",
            "l'erreur  l'epoch 35 est :11.854083858608984\n",
            "l'erreur  l'epoch 36 est :11.235012967591825\n",
            "l'erreur  l'epoch 37 est :11.094970486375665\n",
            "l'erreur  l'epoch 38 est :11.30604166711517\n",
            "l'erreur  l'epoch 39 est :10.689862355669003\n",
            "l'erreur  l'epoch 40 est :10.665650855516343\n",
            "l'erreur  l'epoch 41 est :10.417029551398338\n",
            "l'erreur  l'epoch 42 est :10.001865737615608\n",
            "l'erreur  l'epoch 43 est :10.053629754725685\n",
            "l'erreur  l'epoch 44 est :9.815652255152749\n",
            "l'erreur  l'epoch 45 est :9.543057242072196\n",
            "l'erreur  l'epoch 46 est :9.214265128867858\n",
            "l'erreur  l'epoch 47 est :9.08719195994266\n",
            "l'erreur  l'epoch 48 est :8.987429218484955\n",
            "l'erreur  l'epoch 49 est :8.984554439843198\n",
            "l'erreur  l'epoch 50 est :8.838271650950475\n",
            "l'erreur  l'epoch 51 est :8.85860685131579\n",
            "l'erreur  l'epoch 52 est :8.255646096607503\n",
            "l'erreur  l'epoch 53 est :8.291111568242782\n",
            "l'erreur  l'epoch 54 est :8.17460606422874\n",
            "l'erreur  l'epoch 55 est :8.222418007477136\n",
            "l'erreur  l'epoch 56 est :7.778950270461211\n",
            "l'erreur  l'epoch 57 est :7.909849866628148\n",
            "l'erreur  l'epoch 58 est :7.657451141517871\n",
            "l'erreur  l'epoch 59 est :7.516093388167928\n",
            "l'erreur  l'epoch 60 est :7.377078728026677\n",
            "l'erreur  l'epoch 61 est :7.484354929548335\n",
            "l'erreur  l'epoch 62 est :7.092328824914583\n",
            "l'erreur  l'epoch 63 est :7.0123972381558515\n",
            "l'erreur  l'epoch 64 est :7.187171317986954\n",
            "l'erreur  l'epoch 65 est :6.980778868408162\n",
            "l'erreur  l'epoch 66 est :6.823660863028739\n",
            "l'erreur  l'epoch 67 est :6.711443048055003\n",
            "l'erreur  l'epoch 68 est :6.549742179920643\n",
            "l'erreur  l'epoch 69 est :6.667700656879333\n",
            "l'erreur  l'epoch 70 est :6.4696807837762815\n",
            "l'erreur  l'epoch 71 est :6.284115391588046\n",
            "l'erreur  l'epoch 72 est :6.137425520397283\n",
            "l'erreur  l'epoch 73 est :6.159632788898021\n",
            "l'erreur  l'epoch 74 est :6.209392692053861\n",
            "l'erreur  l'epoch 75 est :6.062014765213791\n",
            "l'erreur  l'epoch 76 est :5.850407462800127\n",
            "l'erreur  l'epoch 77 est :5.986173078043048\n",
            "l'erreur  l'epoch 78 est :5.717723580135959\n",
            "l'erreur  l'epoch 79 est :5.811506593339152\n",
            "l'erreur  l'epoch 80 est :5.495018310192475\n",
            "l'erreur  l'epoch 81 est :5.414909374623429\n",
            "l'erreur  l'epoch 82 est :5.45659381871635\n",
            "l'erreur  l'epoch 83 est :5.261170467533946\n",
            "l'erreur  l'epoch 84 est :5.136881979987294\n",
            "l'erreur  l'epoch 85 est :5.159408550903293\n",
            "l'erreur  l'epoch 86 est :5.1724818257203244\n",
            "l'erreur  l'epoch 87 est :5.066829974183066\n",
            "l'erreur  l'epoch 88 est :5.008963481232219\n",
            "l'erreur  l'epoch 89 est :4.905051183533239\n",
            "l'erreur  l'epoch 90 est :5.099665263730792\n",
            "l'erreur  l'epoch 91 est :4.7820746227768565\n",
            "l'erreur  l'epoch 92 est :4.635849028564842\n",
            "l'erreur  l'epoch 93 est :4.6795521774688345\n",
            "l'erreur  l'epoch 94 est :4.788084510670252\n",
            "l'erreur  l'epoch 95 est :4.583224385339283\n",
            "l'erreur  l'epoch 96 est :4.558221909236829\n",
            "l'erreur  l'epoch 97 est :4.386880443053759\n",
            "l'erreur  l'epoch 98 est :4.398591692946035\n",
            "l'erreur  l'epoch 99 est :4.382408390415455\n",
            "l'erreur  l'epoch 100 est :4.250795168950281\n",
            "l'erreur  l'epoch 101 est :4.184210276068437\n",
            "l'erreur  l'epoch 102 est :4.158649619216923\n",
            "l'erreur  l'epoch 103 est :4.083754981111598\n",
            "l'erreur  l'epoch 104 est :4.060409253774236\n",
            "l'erreur  l'epoch 105 est :4.038011158186354\n",
            "l'erreur  l'epoch 106 est :3.9246807391456344\n",
            "l'erreur  l'epoch 107 est :3.896656397430171\n",
            "l'erreur  l'epoch 108 est :3.9009860947685104\n",
            "l'erreur  l'epoch 109 est :3.8058640826717878\n",
            "l'erreur  l'epoch 110 est :3.742512307998046\n",
            "l'erreur  l'epoch 111 est :3.668870041366683\n",
            "l'erreur  l'epoch 112 est :3.6512726845723464\n",
            "l'erreur  l'epoch 113 est :3.6147822844429855\n",
            "l'erreur  l'epoch 114 est :3.587012338892556\n",
            "l'erreur  l'epoch 115 est :3.53459351000547\n",
            "l'erreur  l'epoch 116 est :3.5876433102122984\n",
            "l'erreur  l'epoch 117 est :3.5118690286101075\n",
            "l'erreur  l'epoch 118 est :3.4463081136181377\n",
            "l'erreur  l'epoch 119 est :3.3579657989733382\n",
            "l'erreur  l'epoch 120 est :3.4021223175604947\n",
            "l'erreur  l'epoch 121 est :3.331305144413092\n",
            "l'erreur  l'epoch 122 est :3.37070854537669\n",
            "l'erreur  l'epoch 123 est :3.3025086007723927\n",
            "l'erreur  l'epoch 124 est :3.174195860289231\n",
            "l'erreur  l'epoch 125 est :3.1513862829089416\n",
            "l'erreur  l'epoch 126 est :3.107692763363029\n",
            "l'erreur  l'epoch 127 est :3.1384082445161874\n",
            "l'erreur  l'epoch 128 est :3.0269993570719573\n",
            "l'erreur  l'epoch 129 est :2.978709998027436\n",
            "l'erreur  l'epoch 130 est :2.952105390323403\n",
            "l'erreur  l'epoch 131 est :2.9738263361539894\n",
            "l'erreur  l'epoch 132 est :2.976405481781775\n",
            "l'erreur  l'epoch 133 est :2.8959331012899088\n",
            "l'erreur  l'epoch 134 est :2.9426568975283285\n",
            "l'erreur  l'epoch 135 est :2.850105126508368\n",
            "l'erreur  l'epoch 136 est :2.835289697045934\n",
            "l'erreur  l'epoch 137 est :2.8022115103010847\n",
            "l'erreur  l'epoch 138 est :2.726810361081021\n",
            "l'erreur  l'epoch 139 est :2.681627982677054\n",
            "l'erreur  l'epoch 140 est :2.7251584254727685\n",
            "l'erreur  l'epoch 141 est :2.640638057364682\n",
            "l'erreur  l'epoch 142 est :2.6546984253992565\n",
            "l'erreur  l'epoch 143 est :2.602442499047895\n",
            "l'erreur  l'epoch 144 est :2.5719727085617734\n",
            "l'erreur  l'epoch 145 est :2.5508403402119044\n",
            "l'erreur  l'epoch 146 est :2.535467020877815\n",
            "l'erreur  l'epoch 147 est :2.486001755974704\n",
            "l'erreur  l'epoch 148 est :2.4789414656242994\n",
            "l'erreur  l'epoch 149 est :2.4285438460725537\n",
            "l'erreur  l'epoch 150 est :2.388159080753395\n",
            "l'erreur  l'epoch 151 est :2.4056185744309357\n",
            "l'erreur  l'epoch 152 est :2.412806257654522\n",
            "l'erreur  l'epoch 153 est :2.2994093029116724\n",
            "l'erreur  l'epoch 154 est :2.3561653576526624\n",
            "l'erreur  l'epoch 155 est :2.303498888127279\n",
            "l'erreur  l'epoch 156 est :2.2532001382266036\n",
            "l'erreur  l'epoch 157 est :2.3091868692732773\n",
            "l'erreur  l'epoch 158 est :2.1906553104770827\n",
            "l'erreur  l'epoch 159 est :2.181719955018413\n",
            "l'erreur  l'epoch 160 est :2.1705554456830196\n",
            "l'erreur  l'epoch 161 est :2.2309698124033943\n",
            "l'erreur  l'epoch 162 est :2.0722739301921442\n",
            "l'erreur  l'epoch 163 est :2.0862844785186634\n",
            "l'erreur  l'epoch 164 est :2.0690940569637353\n",
            "l'erreur  l'epoch 165 est :2.101124157312285\n",
            "l'erreur  l'epoch 166 est :2.082088856951284\n",
            "l'erreur  l'epoch 167 est :2.0574328405678046\n",
            "l'erreur  l'epoch 168 est :2.0718055574518353\n",
            "l'erreur  l'epoch 169 est :2.099489941421749\n",
            "l'erreur  l'epoch 170 est :1.9579963606049955\n",
            "l'erreur  l'epoch 171 est :1.9705870807651171\n",
            "l'erreur  l'epoch 172 est :1.9781204966938846\n",
            "l'erreur  l'epoch 173 est :1.9433859506058373\n",
            "l'erreur  l'epoch 174 est :1.952823346201527\n",
            "l'erreur  l'epoch 175 est :1.8837459906176945\n",
            "l'erreur  l'epoch 176 est :1.893941757803163\n",
            "l'erreur  l'epoch 177 est :1.8434285149450782\n",
            "l'erreur  l'epoch 178 est :1.8332685583451493\n",
            "l'erreur  l'epoch 179 est :1.8095558169314019\n",
            "l'erreur  l'epoch 180 est :1.8010619910493517\n",
            "l'erreur  l'epoch 181 est :1.8728967763541726\n",
            "l'erreur  l'epoch 182 est :1.720605016898043\n",
            "l'erreur  l'epoch 183 est :1.732415041707631\n",
            "l'erreur  l'epoch 184 est :1.7960966872688666\n",
            "l'erreur  l'epoch 185 est :1.7163497049828595\n",
            "l'erreur  l'epoch 186 est :1.7147548237006773\n",
            "l'erreur  l'epoch 187 est :1.7009972230779213\n",
            "l'erreur  l'epoch 188 est :1.69878565244155\n",
            "l'erreur  l'epoch 189 est :1.6511641684681508\n",
            "l'erreur  l'epoch 190 est :1.6366644718423555\n",
            "l'erreur  l'epoch 191 est :1.6557749575740033\n",
            "l'erreur  l'epoch 192 est :1.6347904066136776\n",
            "l'erreur  l'epoch 193 est :1.5904906656709972\n",
            "l'erreur  l'epoch 194 est :1.6387723690275426\n",
            "l'erreur  l'epoch 195 est :1.583892305807899\n",
            "l'erreur  l'epoch 196 est :1.5562470748418462\n",
            "l'erreur  l'epoch 197 est :1.4978538129149745\n",
            "l'erreur  l'epoch 198 est :1.5104342504271786\n",
            "l'erreur  l'epoch 199 est :1.4863836163078956\n",
            "l'erreur  l'epoch 200 est :1.5181101982006024\n",
            "l'erreur  l'epoch 201 est :1.4940742413378645\n",
            "l'erreur  l'epoch 202 est :1.4799653974250773\n",
            "l'erreur  l'epoch 203 est :1.4467181774641549\n",
            "l'erreur  l'epoch 204 est :1.4479873113525061\n",
            "l'erreur  l'epoch 205 est :1.4069666879372407\n",
            "l'erreur  l'epoch 206 est :1.4438298905638178\n",
            "l'erreur  l'epoch 207 est :1.376709453918168\n",
            "l'erreur  l'epoch 208 est :1.4171113459127822\n",
            "l'erreur  l'epoch 209 est :1.4124312918976611\n",
            "l'erreur  l'epoch 210 est :1.4005540689944467\n",
            "l'erreur  l'epoch 211 est :1.3566040100201966\n",
            "l'erreur  l'epoch 212 est :1.3281525617912904\n",
            "l'erreur  l'epoch 213 est :1.3217179590314252\n",
            "l'erreur  l'epoch 214 est :1.3310394865432569\n",
            "l'erreur  l'epoch 215 est :1.3205493322866468\n",
            "l'erreur  l'epoch 216 est :1.3054858238885438\n",
            "l'erreur  l'epoch 217 est :1.3077659723769148\n",
            "l'erreur  l'epoch 218 est :1.2791135532254618\n",
            "l'erreur  l'epoch 219 est :1.302726345660992\n",
            "l'erreur  l'epoch 220 est :1.2700309210695047\n",
            "l'erreur  l'epoch 221 est :1.2601387158834052\n",
            "l'erreur  l'epoch 222 est :1.2285724705958876\n",
            "l'erreur  l'epoch 223 est :1.231751667025027\n",
            "l'erreur  l'epoch 224 est :1.1997884367597862\n",
            "l'erreur  l'epoch 225 est :1.2108274090119566\n",
            "l'erreur  l'epoch 226 est :1.1957922238323604\n",
            "l'erreur  l'epoch 227 est :1.2183971147282642\n",
            "l'erreur  l'epoch 228 est :1.2034380427116242\n",
            "l'erreur  l'epoch 229 est :1.1849176611266439\n",
            "l'erreur  l'epoch 230 est :1.161570546457647\n",
            "l'erreur  l'epoch 231 est :1.170298985216943\n",
            "l'erreur  l'epoch 232 est :1.144226003572632\n",
            "l'erreur  l'epoch 233 est :1.1284051188134967\n",
            "l'erreur  l'epoch 234 est :1.1019593737680249\n",
            "l'erreur  l'epoch 235 est :1.1717220549103518\n",
            "l'erreur  l'epoch 236 est :1.1168879348774463\n",
            "l'erreur  l'epoch 237 est :1.111026265144494\n",
            "l'erreur  l'epoch 238 est :1.1133874343285897\n",
            "l'erreur  l'epoch 239 est :1.10435273324699\n",
            "l'erreur  l'epoch 240 est :1.0424141676096912\n",
            "l'erreur  l'epoch 241 est :1.0517740086748362\n",
            "l'erreur  l'epoch 242 est :1.0850067915118797\n",
            "l'erreur  l'epoch 243 est :1.039241924667125\n",
            "l'erreur  l'epoch 244 est :1.0424656012681262\n",
            "l'erreur  l'epoch 245 est :1.061207664215206\n",
            "l'erreur  l'epoch 246 est :1.0438155705247836\n",
            "l'erreur  l'epoch 247 est :1.047027396373948\n",
            "l'erreur  l'epoch 248 est :1.011901172539026\n",
            "l'erreur  l'epoch 249 est :1.029091459996505\n",
            "l'erreur  l'epoch 250 est :1.00477392839025\n",
            "l'erreur  l'epoch 251 est :1.041742733574737\n",
            "l'erreur  l'epoch 252 est :0.9735113329449795\n",
            "l'erreur  l'epoch 253 est :0.9928525827489635\n",
            "l'erreur  l'epoch 254 est :0.9809091047545592\n",
            "l'erreur  l'epoch 255 est :0.9641307568289317\n",
            "l'erreur  l'epoch 256 est :0.975455590486831\n",
            "l'erreur  l'epoch 257 est :0.9607580899496172\n",
            "l'erreur  l'epoch 258 est :0.9343413775675219\n",
            "l'erreur  l'epoch 259 est :0.9270433398460171\n",
            "l'erreur  l'epoch 260 est :0.9145102307357882\n",
            "l'erreur  l'epoch 261 est :0.9228120091588459\n",
            "l'erreur  l'epoch 262 est :0.9191568449767183\n",
            "l'erreur  l'epoch 263 est :0.9367626700797135\n",
            "l'erreur  l'epoch 264 est :0.8921101849873921\n",
            "l'erreur  l'epoch 265 est :0.8913642294236804\n",
            "l'erreur  l'epoch 266 est :0.8957378225082469\n",
            "l'erreur  l'epoch 267 est :0.9286898555116838\n",
            "l'erreur  l'epoch 268 est :0.8778082576786292\n",
            "l'erreur  l'epoch 269 est :0.8696108804059917\n",
            "l'erreur  l'epoch 270 est :0.8598340208644739\n",
            "l'erreur  l'epoch 271 est :0.8813353012134344\n",
            "l'erreur  l'epoch 272 est :0.8597019880113347\n",
            "l'erreur  l'epoch 273 est :0.8253099334924774\n",
            "l'erreur  l'epoch 274 est :0.835558609732121\n",
            "l'erreur  l'epoch 275 est :0.8359951825423966\n",
            "l'erreur  l'epoch 276 est :0.8386684489138388\n",
            "l'erreur  l'epoch 277 est :0.8061963379982776\n",
            "l'erreur  l'epoch 278 est :0.8207255668557308\n",
            "l'erreur  l'epoch 279 est :0.7959565832120471\n",
            "l'erreur  l'epoch 280 est :0.786065954991423\n",
            "l'erreur  l'epoch 281 est :0.7924924668040522\n",
            "l'erreur  l'epoch 282 est :0.7979211081683972\n",
            "l'erreur  l'epoch 283 est :0.7908202891952721\n",
            "l'erreur  l'epoch 284 est :0.7773939691444439\n",
            "l'erreur  l'epoch 285 est :0.769285910786618\n",
            "l'erreur  l'epoch 286 est :0.7623999954646138\n",
            "l'erreur  l'epoch 287 est :0.7654075632423674\n",
            "l'erreur  l'epoch 288 est :0.7768574634321037\n",
            "l'erreur  l'epoch 289 est :0.7464997796887324\n",
            "l'erreur  l'epoch 290 est :0.7378061196896348\n",
            "l'erreur  l'epoch 291 est :0.7744249070797754\n",
            "l'erreur  l'epoch 292 est :0.7404665124203511\n",
            "l'erreur  l'epoch 293 est :0.7286068211139689\n",
            "l'erreur  l'epoch 294 est :0.7430134924942458\n",
            "l'erreur  l'epoch 295 est :0.7311113180280243\n",
            "l'erreur  l'epoch 296 est :0.7292153080159415\n",
            "l'erreur  l'epoch 297 est :0.6981669038431533\n",
            "l'erreur  l'epoch 298 est :0.7051817890073635\n",
            "l'erreur  l'epoch 299 est :0.6866312256405285\n"
          ]
        }
      ],
      "source": [
        "rbm = RBM(np.size(X_train, 1), 300)\n",
        "error = rbm.train_RBM(X_train, epsilon=0.1, batch_size=32, nb_epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABecAAAE3CAYAAAAg6vr5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPQklEQVR4nO3a0W7kKBRF0TDy//8y8xhp1JPCHTjX4LVeU7KxC3Bly6333r8AAAAAAICYf6oHAAAAAAAAbyPOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAIRdox9sra0cB8Cw3vuPf7dfAU9hvwJ2Yb8CdmG/Anbxab/6+vLmPAAAAAAAxInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhF3VAwDgXXrvw59trU0/Jp+N3nd4khV7y4rzV66vXe5RJfsfCSvWgrkLAHvy5jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAIRd1QM4Se+9eghDWmtDn9vleqgxOo94pl3W9y7jPM3s+26/IOHOPKvcW1ace8Uae+v+W33d9su97bK37DDPVlzPafcInqbyN071njH73CvYA5/Lm/MAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQNhVPQDyeu/VQ+ChWmvVQ+CXrO/PRue5eznPintpv+K/3rxm33ztcLoVv1tGP3vnWVu5D60494p7BAmz5271b4zK9b1C9f0cVTnON+6r3pwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAICwq3oAO+i9Vw+BCVprw59d8Z2Pnn/03NXXA09yZz1UHtNahPusm2ezV3624h6RVTkn33ruatYtT7JiLb55fcPTeHMeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCruoBVOm9D3+2tTb9mDsYve4TVV77inO/+bs8gb3l2VY8T1ad/yTV9x0SZs/dXdbNLnulveU93vr/ILCWPQP4+vLmPAAAAAAAxInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQdlUPoEprbfizvfeFIwH42S771Z1xnqT6ukfPf9qzrPq+83vmZN4OY1zlrXslpMzeX+6sxTfvbXC6Fet7dH85bW/xG+ezqmePN+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACDsqh4Aea216iEAN/Tey85tvzjHne/SnIP7RteNOf5svh/4tmI9VP7GAJ6hem857VlvX52nam54cx4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACLuqB1Cl9149hKlaa9VDGLLivu9y7fC37szx0TX25nUzex96870EvlXuBfZ+YIU7v5nsL0ClXfagFb/ZVly7/5mzvDkPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABhV/UAmKP3Xj0E4AFaa9VDeDz3CNZ58++Rymu3r8Fe7qzZFXvL6PnvnHv0s5XnfjPPiWeq/F4q95ZdnLa33LmeFXs1/8+b8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABA2DX7gL334c+21maffsm571wTc1TOjVVG59GJ1w6rrXj2WLOwlxVrsXJvuWPFMe1tn3lO8Leq/7+sPH/1tcPpZq+xE59hp+1DK36PzD7mLq24ijfnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAg7Jp9wNba7ENykNPmR+/9qHOf9v2wvxXz/LR1C09y5zkyez1Ur6/K83t+z1M9j9jb6Fo0z4BV/CY4wy7/B48e07z8mTfnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAg7KoeAGdorVUPYare+9Dn7lz36DGBb6Nr7M76Om2/WmH2fXfP4e/ssHbsv3Cf/yGAarv8jj9tD1zx/y378+Y8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABB2VQ+AvNZa9RAeb/Qe9d6nn3vFMSvPbb69x+j8uTMndlhjb57jlXvlm+8777FiXz1N5e+mUXfG+ObvEk63w/+Y9qD3qPyuq5/ds699l/91dmlNb9yHvDkPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABhV/UAdtBam3q83vvU4606/+zr5izmxzvc2a9G50T1Hlg5d09bNyu+y9PuEfzGaeuhev/fgd/pnG7F78XKY8LpPLs/q94vfEf78+Y8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABB2VQ+AvNZa9RCAG3rv04+5Yh9YMU7eZ3QeeZbtb/Q7fPPessN6uHPuFdfz5vnB81TOx+q1OPvcK465y2+HyufjnWPucj/5M8/PGiv2q132av6fN+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACDsqh4APFHvvXoIsJQ5zk/MDxIq51lrbehzd8Y4ekz4iXn0TDs8F1eMccUe+Ob9d4d5tMu95PdWrMVKu8zdXcZJljfnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAICwq3oAzNFaqx4CYb5z+Du996nHq16Ls6+n2uj9vHPd1d/Rm+wyH1eMc/SY5uNnu8yjFfsVOZXPkeo5scM+tMMYYWfV+xDwzZvzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQdlUPAAB21nuvHsIrtdaqh8Af3PlerJ3PRu/RivXg+/nMPdpb5XPEmn0+9xP2smLN7vL/hv1qf96cBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAsKt6AG/UWhv+bO996uc4x53v/M6cA4CE0WfT6POu+llX+ZvN78Bnq56bAPYhuK/yN+hpv+3sQT/z5jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAEHZVD+CNeu/Tj9laKz0/eXe+c/ZmffNU9iESzDOSzDd2Ze5+5ncyfBvdM6ybz958jzx75vDmPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhF3VA3ij1lr1EJjEd8muzF2AuUb31d779GOuMDpOzxNgF3f2qzt79ezz2395kup5tmIt8ln19/423pwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMKu6gGQ11qrHgKwiPUN8Gy77NO7jBNghco90P4L305bD733oc+ddt38zJvzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQdlUPAAAAAADgZK216iHwQN6cBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIKz13nv1IAAAAAAA4E28OQ8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGH/ApSM0bb3p2b+AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rbm.generer_image_RBM(nb_data=5, nb_gibbs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DBN:\n",
        "\n",
        "  def __init__(self, config):\n",
        "    self.nb_layers = len(config)\n",
        "    self.rbms = []\n",
        "    for i in range(self.nb_layers - 1):\n",
        "      self.rbms.append(RBM(config[i], config[i+1]))\n",
        "\n",
        "  def train_DBN(self, X, epsilon, batch_size, nb_epochs):\n",
        "    error = []\n",
        "    for i in range(self.nb_layers - 1):\n",
        "      print('Layer' + str(i))\n",
        "      err = self.rbms[i].train_RBM(X, epsilon, batch_size, nb_epochs)\n",
        "      error.append(err)\n",
        "      X = self.rbms[i].entree_sortie_RBM(X)\n",
        "    return error\n",
        "    \n",
        "  def generer_image_DBN(self, nb_data, nb_gibbs):\n",
        "    p = self.rbms[-1].a.shape[0]\n",
        "    q = self.rbms[-1].b.shape[0]\n",
        "    \n",
        "    fig, axs = plt.subplots(1, nb_data, figsize=(15, 5), constrained_layout=True)\n",
        "    for i in range(0, nb_data):\n",
        "        v = (np.random.rand(p) < 0.5) * 1\n",
        "\n",
        "        for iter_gibbs in range(0, nb_gibbs):\n",
        "            h = (np.random.rand(q) < self.rbms[-1].entree_sortie_RBM(v)) * 1\n",
        "            v = (np.random.rand(p) < self.rbms[-1].sortie_entree_RBM(h)) * 1\n",
        "\n",
        "        for j in range(self.nb_layers - 2, -1, -1):\n",
        "            c = self.rbms[j].sortie_entree_RBM(v)\n",
        "            v = (np.random.rand(self.rbms[j].a.shape[0]) < c) * 1     \n",
        "\n",
        "        v = v.reshape(28, 28)\n",
        "        axs[i].imshow(v, cmap='gray')\n",
        "        axs[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "  '''def generer_image_DBN(self, nb_data, nb_gibbs):\n",
        "    p = self.rbms[-1].a.shape[0]\n",
        "    q = self.rbms[-1].b.shape[0]\n",
        "\n",
        "    for i in range(0, nb_data):\n",
        "      v = (np.random.rand(p) < 0.5) * 1\n",
        "\n",
        "      for iter_gibbs in range(0, nb_gibbs):\n",
        "        h = (np.random.rand(q) < self.rbms[-1].entree_sortie_RBM(v)) * 1\n",
        "        v = (np.random.rand(p) < self.rbms[-1].sortie_entree_RBM(h)) * 1\n",
        "\n",
        "      for j in range(self.nb_layers - 2, -1, -1):\n",
        "        c = self.rbms[j].sortie_entree_RBM(v)\n",
        "        v = (np.random.rand(self.rbms[j].a.shape[0]) < c) * 1     \n",
        "     \n",
        "      v = v.reshape(28, 28)\n",
        "      plt.imshow(v, cmap='gray')\n",
        "      plt.show()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = X_train.shape[1]\n",
        "q = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer0\n",
            "l'erreur  l'epoch 0 est :59.75434309707805\n",
            "l'erreur  l'epoch 1 est :49.075232890444184\n",
            "l'erreur  l'epoch 2 est :45.9692434669236\n",
            "l'erreur  l'epoch 3 est :42.43578395101134\n",
            "l'erreur  l'epoch 4 est :38.990821045976055\n",
            "l'erreur  l'epoch 5 est :37.521358701596\n",
            "l'erreur  l'epoch 6 est :36.01959717336292\n",
            "l'erreur  l'epoch 7 est :34.81314545417928\n",
            "l'erreur  l'epoch 8 est :33.77631966899554\n",
            "l'erreur  l'epoch 9 est :32.22409135245946\n",
            "l'erreur  l'epoch 10 est :32.21076423474382\n",
            "l'erreur  l'epoch 11 est :30.365738232193372\n",
            "l'erreur  l'epoch 12 est :30.019592261418126\n",
            "l'erreur  l'epoch 13 est :29.158903521787625\n",
            "l'erreur  l'epoch 14 est :28.943603405303758\n",
            "l'erreur  l'epoch 15 est :27.995070244910572\n",
            "l'erreur  l'epoch 16 est :27.850228852108824\n",
            "l'erreur  l'epoch 17 est :27.237956471895757\n",
            "l'erreur  l'epoch 18 est :26.506953553167573\n",
            "l'erreur  l'epoch 19 est :26.506278043892138\n",
            "l'erreur  l'epoch 20 est :25.63625981663217\n",
            "l'erreur  l'epoch 21 est :25.853991319989706\n",
            "l'erreur  l'epoch 22 est :25.00318133581824\n",
            "l'erreur  l'epoch 23 est :24.767246131337583\n",
            "l'erreur  l'epoch 24 est :24.328754880346388\n",
            "l'erreur  l'epoch 25 est :24.088952017703793\n",
            "l'erreur  l'epoch 26 est :23.889331846765373\n",
            "l'erreur  l'epoch 27 est :23.538831374715187\n",
            "l'erreur  l'epoch 28 est :23.28125236073631\n",
            "l'erreur  l'epoch 29 est :23.141627337996283\n",
            "l'erreur  l'epoch 30 est :22.889433019471763\n",
            "l'erreur  l'epoch 31 est :22.472048803610182\n",
            "l'erreur  l'epoch 32 est :22.444947959668426\n",
            "l'erreur  l'epoch 33 est :22.301201072383623\n",
            "l'erreur  l'epoch 34 est :21.964216733267044\n",
            "l'erreur  l'epoch 35 est :21.884856737540836\n",
            "l'erreur  l'epoch 36 est :21.604107399492026\n",
            "l'erreur  l'epoch 37 est :21.38938201258726\n",
            "l'erreur  l'epoch 38 est :21.349546890373688\n",
            "l'erreur  l'epoch 39 est :21.08309066242384\n",
            "l'erreur  l'epoch 40 est :20.89138504012002\n",
            "l'erreur  l'epoch 41 est :20.80548056064765\n",
            "l'erreur  l'epoch 42 est :20.669348129195708\n",
            "l'erreur  l'epoch 43 est :20.367772919811493\n",
            "l'erreur  l'epoch 44 est :20.43565654557117\n",
            "l'erreur  l'epoch 45 est :20.375639371938806\n",
            "l'erreur  l'epoch 46 est :20.112933468684325\n",
            "l'erreur  l'epoch 47 est :20.13226963982938\n",
            "l'erreur  l'epoch 48 est :19.993783218471336\n",
            "l'erreur  l'epoch 49 est :19.71247362570171\n",
            "l'erreur  l'epoch 50 est :19.500655477299492\n",
            "l'erreur  l'epoch 51 est :19.24193884214905\n",
            "l'erreur  l'epoch 52 est :19.309687905670778\n",
            "l'erreur  l'epoch 53 est :19.241317226891727\n",
            "l'erreur  l'epoch 54 est :19.164803977887114\n",
            "l'erreur  l'epoch 55 est :19.12879702416703\n",
            "l'erreur  l'epoch 56 est :18.846231089625167\n",
            "l'erreur  l'epoch 57 est :18.942149488604482\n",
            "l'erreur  l'epoch 58 est :18.78393320580134\n",
            "l'erreur  l'epoch 59 est :18.643157128330486\n",
            "l'erreur  l'epoch 60 est :18.609520505819297\n",
            "l'erreur  l'epoch 61 est :18.488709629291534\n",
            "l'erreur  l'epoch 62 est :18.44590914591984\n",
            "l'erreur  l'epoch 63 est :18.34166687614627\n",
            "l'erreur  l'epoch 64 est :18.187333974153063\n",
            "l'erreur  l'epoch 65 est :18.434491640684143\n",
            "l'erreur  l'epoch 66 est :18.072210868025543\n",
            "l'erreur  l'epoch 67 est :17.854267621745908\n",
            "l'erreur  l'epoch 68 est :17.83405320637893\n",
            "l'erreur  l'epoch 69 est :17.909397401664716\n",
            "l'erreur  l'epoch 70 est :17.897051845722707\n",
            "l'erreur  l'epoch 71 est :17.72053861131297\n",
            "l'erreur  l'epoch 72 est :17.630899626666075\n",
            "l'erreur  l'epoch 73 est :17.555912156548466\n",
            "l'erreur  l'epoch 74 est :17.33719024736352\n",
            "l'erreur  l'epoch 75 est :17.42018947141277\n",
            "l'erreur  l'epoch 76 est :17.43320294201552\n",
            "l'erreur  l'epoch 77 est :17.37744584988165\n",
            "l'erreur  l'epoch 78 est :17.22247628515534\n",
            "l'erreur  l'epoch 79 est :17.190869168807396\n",
            "l'erreur  l'epoch 80 est :17.03546686710309\n",
            "l'erreur  l'epoch 81 est :17.04214793169151\n",
            "l'erreur  l'epoch 82 est :16.949079974232994\n",
            "l'erreur  l'epoch 83 est :16.89869678252329\n",
            "l'erreur  l'epoch 84 est :17.015099339955988\n",
            "l'erreur  l'epoch 85 est :16.737372018245864\n",
            "l'erreur  l'epoch 86 est :16.907875569307322\n",
            "l'erreur  l'epoch 87 est :16.682983911295192\n",
            "l'erreur  l'epoch 88 est :16.559862431756144\n",
            "l'erreur  l'epoch 89 est :16.543954770350748\n",
            "l'erreur  l'epoch 90 est :16.46391910652995\n",
            "l'erreur  l'epoch 91 est :16.371227686290514\n",
            "l'erreur  l'epoch 92 est :16.538970384723665\n",
            "l'erreur  l'epoch 93 est :16.386656609029977\n",
            "l'erreur  l'epoch 94 est :16.26025079075631\n",
            "l'erreur  l'epoch 95 est :16.311758473634022\n",
            "l'erreur  l'epoch 96 est :16.16803996542681\n",
            "l'erreur  l'epoch 97 est :16.20321578271981\n",
            "l'erreur  l'epoch 98 est :16.052504514655755\n",
            "l'erreur  l'epoch 99 est :15.996244062291884\n",
            "l'erreur  l'epoch 100 est :15.938734187939149\n",
            "l'erreur  l'epoch 101 est :15.937068299351083\n",
            "l'erreur  l'epoch 102 est :15.949386485490825\n",
            "l'erreur  l'epoch 103 est :15.893091577820611\n",
            "l'erreur  l'epoch 104 est :15.792070965868863\n",
            "l'erreur  l'epoch 105 est :15.701673433212731\n",
            "l'erreur  l'epoch 106 est :15.798626919795403\n",
            "l'erreur  l'epoch 107 est :15.749594847765547\n",
            "l'erreur  l'epoch 108 est :15.617309950820303\n",
            "l'erreur  l'epoch 109 est :15.593929340340924\n",
            "l'erreur  l'epoch 110 est :15.636884278473483\n",
            "l'erreur  l'epoch 111 est :15.558522886009726\n",
            "l'erreur  l'epoch 112 est :15.497083847354949\n",
            "l'erreur  l'epoch 113 est :15.436209508858244\n",
            "l'erreur  l'epoch 114 est :15.572187245614858\n",
            "l'erreur  l'epoch 115 est :15.344420696947875\n",
            "l'erreur  l'epoch 116 est :15.323594994200636\n",
            "l'erreur  l'epoch 117 est :15.317155937096874\n",
            "l'erreur  l'epoch 118 est :15.205564961587193\n",
            "l'erreur  l'epoch 119 est :15.242606168711017\n",
            "l'erreur  l'epoch 120 est :15.228345739110933\n",
            "l'erreur  l'epoch 121 est :15.28106509914357\n",
            "l'erreur  l'epoch 122 est :15.188545317873507\n",
            "l'erreur  l'epoch 123 est :15.058102819850916\n",
            "l'erreur  l'epoch 124 est :15.00314465423164\n",
            "l'erreur  l'epoch 125 est :15.021874490490447\n",
            "l'erreur  l'epoch 126 est :14.94527320171294\n",
            "l'erreur  l'epoch 127 est :14.923640864936457\n",
            "l'erreur  l'epoch 128 est :14.865459425933052\n",
            "l'erreur  l'epoch 129 est :14.917379674107497\n",
            "l'erreur  l'epoch 130 est :14.829853011666279\n",
            "l'erreur  l'epoch 131 est :14.866141505839579\n",
            "l'erreur  l'epoch 132 est :14.853318569781125\n",
            "l'erreur  l'epoch 133 est :14.789800712876243\n",
            "l'erreur  l'epoch 134 est :14.733773033567342\n",
            "l'erreur  l'epoch 135 est :14.754360417666867\n",
            "l'erreur  l'epoch 136 est :14.64789562560174\n",
            "l'erreur  l'epoch 137 est :14.575964892773385\n",
            "l'erreur  l'epoch 138 est :14.652015409875284\n",
            "l'erreur  l'epoch 139 est :14.645956741320868\n",
            "l'erreur  l'epoch 140 est :14.590285619866744\n",
            "l'erreur  l'epoch 141 est :14.48859889061408\n",
            "l'erreur  l'epoch 142 est :14.473480118631269\n",
            "l'erreur  l'epoch 143 est :14.46186045809086\n",
            "l'erreur  l'epoch 144 est :14.399118860463389\n",
            "l'erreur  l'epoch 145 est :14.402747404899891\n",
            "l'erreur  l'epoch 146 est :14.364083016076748\n",
            "l'erreur  l'epoch 147 est :14.294888491901567\n",
            "l'erreur  l'epoch 148 est :14.292899543011206\n",
            "l'erreur  l'epoch 149 est :14.311050147509402\n",
            "l'erreur  l'epoch 150 est :14.255398853972544\n",
            "l'erreur  l'epoch 151 est :14.239270714843968\n",
            "l'erreur  l'epoch 152 est :14.25244130410965\n",
            "l'erreur  l'epoch 153 est :14.240850674164832\n",
            "l'erreur  l'epoch 154 est :14.178953295698433\n",
            "l'erreur  l'epoch 155 est :14.098946507377557\n",
            "l'erreur  l'epoch 156 est :14.20113690596854\n",
            "l'erreur  l'epoch 157 est :14.056484380983905\n",
            "l'erreur  l'epoch 158 est :13.946671283378313\n",
            "l'erreur  l'epoch 159 est :14.004360464022104\n",
            "l'erreur  l'epoch 160 est :13.977766117587091\n",
            "l'erreur  l'epoch 161 est :14.027406366743872\n",
            "l'erreur  l'epoch 162 est :14.032826440332043\n",
            "l'erreur  l'epoch 163 est :14.190072187352015\n",
            "l'erreur  l'epoch 164 est :13.86416696955154\n",
            "l'erreur  l'epoch 165 est :13.86532140877322\n",
            "l'erreur  l'epoch 166 est :13.905703421319718\n",
            "l'erreur  l'epoch 167 est :13.855647889862851\n",
            "l'erreur  l'epoch 168 est :13.738718252755149\n",
            "l'erreur  l'epoch 169 est :13.843650580683878\n",
            "l'erreur  l'epoch 170 est :13.784562462829268\n",
            "l'erreur  l'epoch 171 est :13.743394321677549\n",
            "l'erreur  l'epoch 172 est :13.688100366963265\n",
            "l'erreur  l'epoch 173 est :13.767955191935181\n",
            "l'erreur  l'epoch 174 est :13.813877289582932\n",
            "l'erreur  l'epoch 175 est :13.681989564200673\n",
            "l'erreur  l'epoch 176 est :13.634218192823665\n",
            "l'erreur  l'epoch 177 est :13.64684880873279\n",
            "l'erreur  l'epoch 178 est :13.658417381791203\n",
            "l'erreur  l'epoch 179 est :13.539832124030047\n",
            "l'erreur  l'epoch 180 est :13.425437097433742\n",
            "l'erreur  l'epoch 181 est :13.544502691471576\n",
            "l'erreur  l'epoch 182 est :13.48912380220079\n",
            "l'erreur  l'epoch 183 est :13.50446057000789\n",
            "l'erreur  l'epoch 184 est :13.517385460425405\n",
            "l'erreur  l'epoch 185 est :13.437292057716814\n",
            "l'erreur  l'epoch 186 est :13.4374830650925\n",
            "l'erreur  l'epoch 187 est :13.421574503061823\n",
            "l'erreur  l'epoch 188 est :13.383967662892266\n",
            "l'erreur  l'epoch 189 est :13.435095556441981\n",
            "l'erreur  l'epoch 190 est :13.396716165240209\n",
            "l'erreur  l'epoch 191 est :13.23982007474647\n",
            "l'erreur  l'epoch 192 est :13.364874192589445\n",
            "l'erreur  l'epoch 193 est :13.340063739366917\n",
            "l'erreur  l'epoch 194 est :13.288979855184127\n",
            "l'erreur  l'epoch 195 est :13.26740550311878\n",
            "l'erreur  l'epoch 196 est :13.247903713064154\n",
            "l'erreur  l'epoch 197 est :13.263390181242533\n",
            "l'erreur  l'epoch 198 est :13.166630767152505\n",
            "l'erreur  l'epoch 199 est :13.231960293702688\n",
            "l'erreur  l'epoch 200 est :13.082752562182343\n",
            "l'erreur  l'epoch 201 est :13.151717723488368\n",
            "l'erreur  l'epoch 202 est :13.066337913383324\n",
            "l'erreur  l'epoch 203 est :13.128185873607217\n",
            "l'erreur  l'epoch 204 est :13.028160870826369\n",
            "l'erreur  l'epoch 205 est :13.125622764202129\n",
            "l'erreur  l'epoch 206 est :13.1158239532907\n",
            "l'erreur  l'epoch 207 est :13.038382490682741\n",
            "l'erreur  l'epoch 208 est :13.045764532966281\n",
            "l'erreur  l'epoch 209 est :12.929120351881716\n",
            "l'erreur  l'epoch 210 est :13.076908509317695\n",
            "l'erreur  l'epoch 211 est :12.98781133382286\n",
            "l'erreur  l'epoch 212 est :12.895022424213142\n",
            "l'erreur  l'epoch 213 est :12.930433780484378\n",
            "l'erreur  l'epoch 214 est :12.893414398967883\n",
            "l'erreur  l'epoch 215 est :12.90356664147992\n",
            "l'erreur  l'epoch 216 est :12.876501701699919\n",
            "l'erreur  l'epoch 217 est :12.887601916696832\n",
            "l'erreur  l'epoch 218 est :12.752791825797873\n",
            "l'erreur  l'epoch 219 est :12.842167145431256\n",
            "l'erreur  l'epoch 220 est :12.907055572273812\n",
            "l'erreur  l'epoch 221 est :12.786794096004538\n",
            "l'erreur  l'epoch 222 est :12.82564802258477\n",
            "l'erreur  l'epoch 223 est :12.762578058632219\n",
            "l'erreur  l'epoch 224 est :12.813316984950868\n",
            "l'erreur  l'epoch 225 est :12.745432702733972\n",
            "l'erreur  l'epoch 226 est :12.691350899667112\n",
            "l'erreur  l'epoch 227 est :12.682434911935008\n",
            "l'erreur  l'epoch 228 est :12.677084964813474\n",
            "l'erreur  l'epoch 229 est :12.667532886428356\n",
            "l'erreur  l'epoch 230 est :12.651557361017565\n",
            "l'erreur  l'epoch 231 est :12.624450317674633\n",
            "l'erreur  l'epoch 232 est :12.63221305258054\n",
            "l'erreur  l'epoch 233 est :12.646644411565106\n",
            "l'erreur  l'epoch 234 est :12.642694145331017\n",
            "l'erreur  l'epoch 235 est :12.579929553610805\n",
            "l'erreur  l'epoch 236 est :12.579354245601191\n",
            "l'erreur  l'epoch 237 est :12.571086778213244\n",
            "l'erreur  l'epoch 238 est :12.536457306288279\n",
            "l'erreur  l'epoch 239 est :12.621315454952901\n",
            "l'erreur  l'epoch 240 est :12.617260984832665\n",
            "l'erreur  l'epoch 241 est :12.567853449468554\n",
            "l'erreur  l'epoch 242 est :12.483532207004146\n",
            "l'erreur  l'epoch 243 est :12.46600580833546\n",
            "l'erreur  l'epoch 244 est :12.398984843281355\n",
            "l'erreur  l'epoch 245 est :12.521889785122283\n",
            "l'erreur  l'epoch 246 est :12.418759085245403\n",
            "l'erreur  l'epoch 247 est :12.43196758794252\n",
            "l'erreur  l'epoch 248 est :12.45275983107604\n",
            "l'erreur  l'epoch 249 est :12.508184331128723\n",
            "l'erreur  l'epoch 250 est :12.337012241819519\n",
            "l'erreur  l'epoch 251 est :12.409391871157416\n",
            "l'erreur  l'epoch 252 est :12.36062359773731\n",
            "l'erreur  l'epoch 253 est :12.343932917093582\n",
            "l'erreur  l'epoch 254 est :12.390785117020929\n",
            "l'erreur  l'epoch 255 est :12.422612218385996\n",
            "l'erreur  l'epoch 256 est :12.345600403432552\n",
            "l'erreur  l'epoch 257 est :12.260148247689571\n",
            "l'erreur  l'epoch 258 est :12.288091211440893\n",
            "l'erreur  l'epoch 259 est :12.330118781435521\n",
            "l'erreur  l'epoch 260 est :12.225582061804388\n",
            "l'erreur  l'epoch 261 est :12.316351044551595\n",
            "l'erreur  l'epoch 262 est :12.19955759873617\n",
            "l'erreur  l'epoch 263 est :12.20396853494413\n",
            "l'erreur  l'epoch 264 est :12.252623620824092\n",
            "l'erreur  l'epoch 265 est :12.177281046042433\n",
            "l'erreur  l'epoch 266 est :12.272248506752065\n",
            "l'erreur  l'epoch 267 est :12.095676564847423\n",
            "l'erreur  l'epoch 268 est :12.14853137881863\n",
            "l'erreur  l'epoch 269 est :12.140801163559749\n",
            "l'erreur  l'epoch 270 est :12.147203773062417\n",
            "l'erreur  l'epoch 271 est :12.174806436892938\n",
            "l'erreur  l'epoch 272 est :12.157292363890118\n",
            "l'erreur  l'epoch 273 est :12.089375226173786\n",
            "l'erreur  l'epoch 274 est :12.095393110073433\n",
            "l'erreur  l'epoch 275 est :12.099514755244723\n",
            "l'erreur  l'epoch 276 est :12.112772732522792\n",
            "l'erreur  l'epoch 277 est :12.068194523559171\n",
            "l'erreur  l'epoch 278 est :12.068275062842464\n",
            "l'erreur  l'epoch 279 est :11.981402317977278\n",
            "l'erreur  l'epoch 280 est :12.092509909626777\n",
            "l'erreur  l'epoch 281 est :11.989962918928368\n",
            "l'erreur  l'epoch 282 est :12.041520516717542\n",
            "l'erreur  l'epoch 283 est :12.03303160846114\n",
            "l'erreur  l'epoch 284 est :11.951902629650244\n",
            "l'erreur  l'epoch 285 est :11.946127340106255\n",
            "l'erreur  l'epoch 286 est :12.015706654788863\n",
            "l'erreur  l'epoch 287 est :11.985576459537688\n",
            "l'erreur  l'epoch 288 est :12.00185517108364\n",
            "l'erreur  l'epoch 289 est :11.98996572209852\n",
            "l'erreur  l'epoch 290 est :11.920720483654751\n",
            "l'erreur  l'epoch 291 est :11.9469975240219\n",
            "l'erreur  l'epoch 292 est :11.883189954992842\n",
            "l'erreur  l'epoch 293 est :11.908054988875525\n",
            "l'erreur  l'epoch 294 est :11.846622290695779\n",
            "l'erreur  l'epoch 295 est :11.839038845683415\n",
            "l'erreur  l'epoch 296 est :11.88126320391439\n",
            "l'erreur  l'epoch 297 est :11.822523298176172\n",
            "l'erreur  l'epoch 298 est :11.845616538595236\n",
            "l'erreur  l'epoch 299 est :11.811798967450649\n",
            "Layer1\n",
            "l'erreur  l'epoch 0 est :17.81213273371709\n",
            "l'erreur  l'epoch 1 est :14.351537116969299\n",
            "l'erreur  l'epoch 2 est :12.744324489001842\n",
            "l'erreur  l'epoch 3 est :11.746963847187587\n",
            "l'erreur  l'epoch 4 est :10.824854336415436\n",
            "l'erreur  l'epoch 5 est :10.385521785667315\n",
            "l'erreur  l'epoch 6 est :9.818485220311429\n",
            "l'erreur  l'epoch 7 est :9.399879688551575\n",
            "l'erreur  l'epoch 8 est :9.21688657340999\n",
            "l'erreur  l'epoch 9 est :8.680503525561663\n",
            "l'erreur  l'epoch 10 est :8.388487556877125\n",
            "l'erreur  l'epoch 11 est :8.113860190867541\n",
            "l'erreur  l'epoch 12 est :7.994217523956545\n",
            "l'erreur  l'epoch 13 est :7.753162154846131\n",
            "l'erreur  l'epoch 14 est :7.518844951901056\n",
            "l'erreur  l'epoch 15 est :7.1724319727759775\n",
            "l'erreur  l'epoch 16 est :7.155994590579948\n",
            "l'erreur  l'epoch 17 est :6.987861388998618\n",
            "l'erreur  l'epoch 18 est :6.745838264007434\n",
            "l'erreur  l'epoch 19 est :6.613490490605051\n",
            "l'erreur  l'epoch 20 est :6.441462196088682\n",
            "l'erreur  l'epoch 21 est :6.3842431805690945\n",
            "l'erreur  l'epoch 22 est :6.282435802003184\n",
            "l'erreur  l'epoch 23 est :6.144880100016282\n",
            "l'erreur  l'epoch 24 est :6.060566334601063\n",
            "l'erreur  l'epoch 25 est :5.888965618077764\n",
            "l'erreur  l'epoch 26 est :5.7101781164248475\n",
            "l'erreur  l'epoch 27 est :5.668743765736819\n",
            "l'erreur  l'epoch 28 est :5.504104721506969\n",
            "l'erreur  l'epoch 29 est :5.412587816371364\n",
            "l'erreur  l'epoch 30 est :5.299063705774209\n",
            "l'erreur  l'epoch 31 est :5.2632904147357955\n",
            "l'erreur  l'epoch 32 est :5.249524548410684\n",
            "l'erreur  l'epoch 33 est :5.258987516068903\n",
            "l'erreur  l'epoch 34 est :5.069934268575975\n",
            "l'erreur  l'epoch 35 est :5.062706741419823\n",
            "l'erreur  l'epoch 36 est :5.027735130914942\n",
            "l'erreur  l'epoch 37 est :4.899100801460416\n",
            "l'erreur  l'epoch 38 est :4.95885303396403\n",
            "l'erreur  l'epoch 39 est :4.747989648574049\n",
            "l'erreur  l'epoch 40 est :4.742078836763604\n",
            "l'erreur  l'epoch 41 est :4.699052293946846\n",
            "l'erreur  l'epoch 42 est :4.627249597606313\n",
            "l'erreur  l'epoch 43 est :4.524960029145144\n",
            "l'erreur  l'epoch 44 est :4.592352995870081\n",
            "l'erreur  l'epoch 45 est :4.475304098471345\n",
            "l'erreur  l'epoch 46 est :4.588898005217101\n",
            "l'erreur  l'epoch 47 est :4.390849374966011\n",
            "l'erreur  l'epoch 48 est :4.363461983289248\n",
            "l'erreur  l'epoch 49 est :4.280734203261278\n",
            "l'erreur  l'epoch 50 est :4.230944866617284\n",
            "l'erreur  l'epoch 51 est :4.287001755445956\n",
            "l'erreur  l'epoch 52 est :4.164041481480228\n",
            "l'erreur  l'epoch 53 est :4.129536441795676\n",
            "l'erreur  l'epoch 54 est :4.065658111592645\n",
            "l'erreur  l'epoch 55 est :4.047967951112378\n",
            "l'erreur  l'epoch 56 est :3.9334096868432367\n",
            "l'erreur  l'epoch 57 est :3.9680790297660065\n",
            "l'erreur  l'epoch 58 est :4.044261305758492\n",
            "l'erreur  l'epoch 59 est :3.8682819743229926\n",
            "l'erreur  l'epoch 60 est :3.885857360705616\n",
            "l'erreur  l'epoch 61 est :3.9211397961815653\n",
            "l'erreur  l'epoch 62 est :3.8197869665448003\n",
            "l'erreur  l'epoch 63 est :3.783028750739005\n",
            "l'erreur  l'epoch 64 est :3.6897808902536977\n",
            "l'erreur  l'epoch 65 est :3.6426656115440257\n",
            "l'erreur  l'epoch 66 est :3.688052984608246\n",
            "l'erreur  l'epoch 67 est :3.617658353483292\n",
            "l'erreur  l'epoch 68 est :3.580699125279436\n",
            "l'erreur  l'epoch 69 est :3.593700944632418\n",
            "l'erreur  l'epoch 70 est :3.7103186522652445\n",
            "l'erreur  l'epoch 71 est :3.5128067498471243\n",
            "l'erreur  l'epoch 72 est :3.4695797168449958\n",
            "l'erreur  l'epoch 73 est :3.5454207643324476\n",
            "l'erreur  l'epoch 74 est :3.5029971432224145\n",
            "l'erreur  l'epoch 75 est :3.4201189638543106\n",
            "l'erreur  l'epoch 76 est :3.417166261492098\n",
            "l'erreur  l'epoch 77 est :3.479784309769042\n",
            "l'erreur  l'epoch 78 est :3.3310668655813727\n",
            "l'erreur  l'epoch 79 est :3.3668810505676126\n",
            "l'erreur  l'epoch 80 est :3.3089955992957027\n",
            "l'erreur  l'epoch 81 est :3.312734041663903\n",
            "l'erreur  l'epoch 82 est :3.3309594164343235\n",
            "l'erreur  l'epoch 83 est :3.1921015585917503\n",
            "l'erreur  l'epoch 84 est :3.1983365266178514\n",
            "l'erreur  l'epoch 85 est :3.1517470475356024\n",
            "l'erreur  l'epoch 86 est :3.214671735599425\n",
            "l'erreur  l'epoch 87 est :3.1608485433550157\n",
            "l'erreur  l'epoch 88 est :3.1421944251223977\n",
            "l'erreur  l'epoch 89 est :3.095698794876988\n",
            "l'erreur  l'epoch 90 est :3.176785410145936\n",
            "l'erreur  l'epoch 91 est :3.125817708590645\n",
            "l'erreur  l'epoch 92 est :3.0646689754840044\n",
            "l'erreur  l'epoch 93 est :3.0304083716404215\n",
            "l'erreur  l'epoch 94 est :3.084264121799376\n",
            "l'erreur  l'epoch 95 est :3.0047754341976542\n",
            "l'erreur  l'epoch 96 est :3.0009325485702822\n",
            "l'erreur  l'epoch 97 est :2.9746502471332392\n",
            "l'erreur  l'epoch 98 est :2.951128205638782\n",
            "l'erreur  l'epoch 99 est :2.9854956219182003\n",
            "l'erreur  l'epoch 100 est :2.917153964793691\n",
            "l'erreur  l'epoch 101 est :2.9085184622258877\n",
            "l'erreur  l'epoch 102 est :2.8774502295123217\n",
            "l'erreur  l'epoch 103 est :2.8371001052827727\n",
            "l'erreur  l'epoch 104 est :2.8800403971319026\n",
            "l'erreur  l'epoch 105 est :2.892389517451527\n",
            "l'erreur  l'epoch 106 est :2.8532161045896816\n",
            "l'erreur  l'epoch 107 est :2.814763883911827\n",
            "l'erreur  l'epoch 108 est :2.867945923216942\n",
            "l'erreur  l'epoch 109 est :2.8911825446438773\n",
            "l'erreur  l'epoch 110 est :2.7788135669890384\n",
            "l'erreur  l'epoch 111 est :2.7956311968779803\n",
            "l'erreur  l'epoch 112 est :2.704400738892672\n",
            "l'erreur  l'epoch 113 est :2.817423346308863\n",
            "l'erreur  l'epoch 114 est :2.7236394346886525\n",
            "l'erreur  l'epoch 115 est :2.7866893233906707\n",
            "l'erreur  l'epoch 116 est :2.729882569749647\n",
            "l'erreur  l'epoch 117 est :2.697399235299787\n",
            "l'erreur  l'epoch 118 est :2.6564694074568838\n",
            "l'erreur  l'epoch 119 est :2.6491080718303186\n",
            "l'erreur  l'epoch 120 est :2.6671465771785545\n",
            "l'erreur  l'epoch 121 est :2.615736058974786\n",
            "l'erreur  l'epoch 122 est :2.639390305096415\n",
            "l'erreur  l'epoch 123 est :2.654133341531411\n",
            "l'erreur  l'epoch 124 est :2.6027367930721903\n",
            "l'erreur  l'epoch 125 est :2.639781320904804\n",
            "l'erreur  l'epoch 126 est :2.525811689510756\n",
            "l'erreur  l'epoch 127 est :2.6059386315187507\n",
            "l'erreur  l'epoch 128 est :2.579971756911923\n",
            "l'erreur  l'epoch 129 est :2.642785863250427\n",
            "l'erreur  l'epoch 130 est :2.534939038813907\n",
            "l'erreur  l'epoch 131 est :2.5730898897835415\n",
            "l'erreur  l'epoch 132 est :2.533674060563172\n",
            "l'erreur  l'epoch 133 est :2.4568525214457426\n",
            "l'erreur  l'epoch 134 est :2.499455749604815\n",
            "l'erreur  l'epoch 135 est :2.50021046240158\n",
            "l'erreur  l'epoch 136 est :2.459321473444262\n",
            "l'erreur  l'epoch 137 est :2.4749669765441156\n",
            "l'erreur  l'epoch 138 est :2.4796694645711175\n",
            "l'erreur  l'epoch 139 est :2.4797897895318064\n",
            "l'erreur  l'epoch 140 est :2.4570804882746726\n",
            "l'erreur  l'epoch 141 est :2.4442163505958243\n",
            "l'erreur  l'epoch 142 est :2.429353062673543\n",
            "l'erreur  l'epoch 143 est :2.414405426297202\n",
            "l'erreur  l'epoch 144 est :2.437391848101746\n",
            "l'erreur  l'epoch 145 est :2.3569376008987573\n",
            "l'erreur  l'epoch 146 est :2.4146641257967314\n",
            "l'erreur  l'epoch 147 est :2.3474484081270792\n",
            "l'erreur  l'epoch 148 est :2.3851579768312554\n",
            "l'erreur  l'epoch 149 est :2.377155040818342\n",
            "l'erreur  l'epoch 150 est :2.3462058542487574\n",
            "l'erreur  l'epoch 151 est :2.385586833492932\n",
            "l'erreur  l'epoch 152 est :2.3707543750736746\n",
            "l'erreur  l'epoch 153 est :2.303408767316325\n",
            "l'erreur  l'epoch 154 est :2.3412200449562377\n",
            "l'erreur  l'epoch 155 est :2.338924172986414\n",
            "l'erreur  l'epoch 156 est :2.348626642683698\n",
            "l'erreur  l'epoch 157 est :2.344534108820925\n",
            "l'erreur  l'epoch 158 est :2.291834919320784\n",
            "l'erreur  l'epoch 159 est :2.298096849929999\n",
            "l'erreur  l'epoch 160 est :2.286437767445569\n",
            "l'erreur  l'epoch 161 est :2.2693229749678396\n",
            "l'erreur  l'epoch 162 est :2.284451121118434\n",
            "l'erreur  l'epoch 163 est :2.2645353614969665\n",
            "l'erreur  l'epoch 164 est :2.2543569283452927\n",
            "l'erreur  l'epoch 165 est :2.2328253947408743\n",
            "l'erreur  l'epoch 166 est :2.248469858231522\n",
            "l'erreur  l'epoch 167 est :2.231817318650924\n",
            "l'erreur  l'epoch 168 est :2.2150556839911477\n",
            "l'erreur  l'epoch 169 est :2.2246633578074024\n",
            "l'erreur  l'epoch 170 est :2.1916651458607306\n",
            "l'erreur  l'epoch 171 est :2.212175733026025\n",
            "l'erreur  l'epoch 172 est :2.19368857586204\n",
            "l'erreur  l'epoch 173 est :2.193011878432819\n",
            "l'erreur  l'epoch 174 est :2.1625314890462715\n",
            "l'erreur  l'epoch 175 est :2.186080058327508\n",
            "l'erreur  l'epoch 176 est :2.1715711697406683\n",
            "l'erreur  l'epoch 177 est :2.173815770735719\n",
            "l'erreur  l'epoch 178 est :2.1440843800166025\n",
            "l'erreur  l'epoch 179 est :2.1705152294548014\n",
            "l'erreur  l'epoch 180 est :2.1728043519621694\n",
            "l'erreur  l'epoch 181 est :2.1241556601226894\n",
            "l'erreur  l'epoch 182 est :2.125369920536115\n",
            "l'erreur  l'epoch 183 est :2.157473571693825\n",
            "l'erreur  l'epoch 184 est :2.102519047930227\n",
            "l'erreur  l'epoch 185 est :2.1764758571689558\n",
            "l'erreur  l'epoch 186 est :2.093426111988616\n",
            "l'erreur  l'epoch 187 est :2.0604382941918566\n",
            "l'erreur  l'epoch 188 est :2.0564037565045052\n",
            "l'erreur  l'epoch 189 est :2.109366307022663\n",
            "l'erreur  l'epoch 190 est :2.06400260046676\n",
            "l'erreur  l'epoch 191 est :2.061174711652312\n",
            "l'erreur  l'epoch 192 est :2.074940532989009\n",
            "l'erreur  l'epoch 193 est :2.0393540327348765\n",
            "l'erreur  l'epoch 194 est :2.0510908162064303\n",
            "l'erreur  l'epoch 195 est :2.0304626092957547\n",
            "l'erreur  l'epoch 196 est :2.0377893105080322\n",
            "l'erreur  l'epoch 197 est :2.0556334239115475\n",
            "l'erreur  l'epoch 198 est :2.025258360911095\n",
            "l'erreur  l'epoch 199 est :2.0680225162481403\n",
            "l'erreur  l'epoch 200 est :2.0604343734653163\n",
            "l'erreur  l'epoch 201 est :2.0075915647487057\n",
            "l'erreur  l'epoch 202 est :2.056651125651542\n",
            "l'erreur  l'epoch 203 est :2.035762593080099\n",
            "l'erreur  l'epoch 204 est :2.0178078200305944\n",
            "l'erreur  l'epoch 205 est :1.9998884349598578\n",
            "l'erreur  l'epoch 206 est :1.9505858234800268\n",
            "l'erreur  l'epoch 207 est :2.016221578011289\n",
            "l'erreur  l'epoch 208 est :1.9669264851950907\n",
            "l'erreur  l'epoch 209 est :2.00238001990496\n",
            "l'erreur  l'epoch 210 est :1.978047550988994\n",
            "l'erreur  l'epoch 211 est :1.9984546155384735\n",
            "l'erreur  l'epoch 212 est :1.9553368725979856\n",
            "l'erreur  l'epoch 213 est :1.944350348252164\n",
            "l'erreur  l'epoch 214 est :1.952867299564071\n",
            "l'erreur  l'epoch 215 est :1.9438090070843264\n",
            "l'erreur  l'epoch 216 est :1.9823437282799345\n",
            "l'erreur  l'epoch 217 est :1.9544851197590825\n",
            "l'erreur  l'epoch 218 est :1.9814785828839694\n",
            "l'erreur  l'epoch 219 est :1.9392608356313483\n",
            "l'erreur  l'epoch 220 est :1.9639307051693597\n",
            "l'erreur  l'epoch 221 est :1.9387407042853124\n",
            "l'erreur  l'epoch 222 est :1.9106001772049086\n",
            "l'erreur  l'epoch 223 est :1.9196868063502572\n",
            "l'erreur  l'epoch 224 est :1.8908873315708543\n",
            "l'erreur  l'epoch 225 est :1.900869691435566\n",
            "l'erreur  l'epoch 226 est :1.8898559672364013\n",
            "l'erreur  l'epoch 227 est :1.9240992606608502\n",
            "l'erreur  l'epoch 228 est :1.8792947550321941\n",
            "l'erreur  l'epoch 229 est :1.9503041628198061\n",
            "l'erreur  l'epoch 230 est :1.895945178996071\n",
            "l'erreur  l'epoch 231 est :1.9158316274857277\n",
            "l'erreur  l'epoch 232 est :1.8766040764034826\n",
            "l'erreur  l'epoch 233 est :1.8813702434901478\n",
            "l'erreur  l'epoch 234 est :1.9056998712110267\n",
            "l'erreur  l'epoch 235 est :1.8531532890462488\n",
            "l'erreur  l'epoch 236 est :1.870978763326729\n",
            "l'erreur  l'epoch 237 est :1.871188957106641\n",
            "l'erreur  l'epoch 238 est :1.844918105089346\n",
            "l'erreur  l'epoch 239 est :1.8523528105871985\n",
            "l'erreur  l'epoch 240 est :1.8314465685442305\n",
            "l'erreur  l'epoch 241 est :1.866405498143521\n",
            "l'erreur  l'epoch 242 est :1.8648818956721136\n",
            "l'erreur  l'epoch 243 est :1.8219568342032508\n",
            "l'erreur  l'epoch 244 est :1.8258369601348068\n",
            "l'erreur  l'epoch 245 est :1.8298851663423095\n",
            "l'erreur  l'epoch 246 est :1.8227701529902247\n",
            "l'erreur  l'epoch 247 est :1.7965973420268155\n",
            "l'erreur  l'epoch 248 est :1.8225536134501985\n",
            "l'erreur  l'epoch 249 est :1.8122873152180454\n",
            "l'erreur  l'epoch 250 est :1.768611404560254\n",
            "l'erreur  l'epoch 251 est :1.8169876160374336\n",
            "l'erreur  l'epoch 252 est :1.7889441592055888\n",
            "l'erreur  l'epoch 253 est :1.7767462779700849\n",
            "l'erreur  l'epoch 254 est :1.7805008883359676\n",
            "l'erreur  l'epoch 255 est :1.7859754162448553\n",
            "l'erreur  l'epoch 256 est :1.783865259641436\n",
            "l'erreur  l'epoch 257 est :1.7810522511626232\n",
            "l'erreur  l'epoch 258 est :1.7790662121459555\n",
            "l'erreur  l'epoch 259 est :1.774327427339515\n",
            "l'erreur  l'epoch 260 est :1.7657066164525195\n",
            "l'erreur  l'epoch 261 est :1.7755525134146026\n",
            "l'erreur  l'epoch 262 est :1.768765582120184\n",
            "l'erreur  l'epoch 263 est :1.778546795232507\n",
            "l'erreur  l'epoch 264 est :1.744470230138545\n",
            "l'erreur  l'epoch 265 est :1.7558687799293617\n",
            "l'erreur  l'epoch 266 est :1.749995530601476\n",
            "l'erreur  l'epoch 267 est :1.759410269822272\n",
            "l'erreur  l'epoch 268 est :1.7547719971809814\n",
            "l'erreur  l'epoch 269 est :1.7837937810973132\n",
            "l'erreur  l'epoch 270 est :1.7812454660473023\n",
            "l'erreur  l'epoch 271 est :1.7173809480597997\n",
            "l'erreur  l'epoch 272 est :1.7321287635414564\n",
            "l'erreur  l'epoch 273 est :1.740651996797068\n",
            "l'erreur  l'epoch 274 est :1.7427779587802112\n",
            "l'erreur  l'epoch 275 est :1.7481546457692516\n",
            "l'erreur  l'epoch 276 est :1.7268863104260919\n",
            "l'erreur  l'epoch 277 est :1.7295676135026323\n",
            "l'erreur  l'epoch 278 est :1.757293751729568\n",
            "l'erreur  l'epoch 279 est :1.7371292602474595\n",
            "l'erreur  l'epoch 280 est :1.697007136809395\n",
            "l'erreur  l'epoch 281 est :1.681062095740893\n",
            "l'erreur  l'epoch 282 est :1.7195372120961636\n",
            "l'erreur  l'epoch 283 est :1.7070340467253846\n",
            "l'erreur  l'epoch 284 est :1.7056273240347826\n",
            "l'erreur  l'epoch 285 est :1.7114488862081378\n",
            "l'erreur  l'epoch 286 est :1.7042216833054948\n",
            "l'erreur  l'epoch 287 est :1.7310263986320717\n",
            "l'erreur  l'epoch 288 est :1.7584398567025454\n",
            "l'erreur  l'epoch 289 est :1.6908990200066834\n",
            "l'erreur  l'epoch 290 est :1.674892874041072\n",
            "l'erreur  l'epoch 291 est :1.6974598201035827\n",
            "l'erreur  l'epoch 292 est :1.6975755096165013\n",
            "l'erreur  l'epoch 293 est :1.6949278844840376\n",
            "l'erreur  l'epoch 294 est :1.6642099585399028\n",
            "l'erreur  l'epoch 295 est :1.6674621756750923\n",
            "l'erreur  l'epoch 296 est :1.669336373459019\n",
            "l'erreur  l'epoch 297 est :1.6727807200346296\n",
            "l'erreur  l'epoch 298 est :1.6630187905364417\n",
            "l'erreur  l'epoch 299 est :1.683780588075908\n",
            "Layer2\n",
            "l'erreur  l'epoch 0 est :14.713786404327767\n",
            "l'erreur  l'epoch 1 est :10.714745672881977\n",
            "l'erreur  l'epoch 2 est :9.525406462027105\n",
            "l'erreur  l'epoch 3 est :8.305113778509526\n",
            "l'erreur  l'epoch 4 est :7.332301859285397\n",
            "l'erreur  l'epoch 5 est :6.819063447963484\n",
            "l'erreur  l'epoch 6 est :6.5418820410616805\n",
            "l'erreur  l'epoch 7 est :6.205852573816197\n",
            "l'erreur  l'epoch 8 est :5.999587523030272\n",
            "l'erreur  l'epoch 9 est :5.736351340174865\n",
            "l'erreur  l'epoch 10 est :5.525883123504724\n",
            "l'erreur  l'epoch 11 est :5.634076965507942\n",
            "l'erreur  l'epoch 12 est :5.221809292321645\n",
            "l'erreur  l'epoch 13 est :4.970463304226002\n",
            "l'erreur  l'epoch 14 est :4.871746943075689\n",
            "l'erreur  l'epoch 15 est :4.757622080457846\n",
            "l'erreur  l'epoch 16 est :4.631325501324465\n",
            "l'erreur  l'epoch 17 est :4.646828866816646\n",
            "l'erreur  l'epoch 18 est :4.424041077576316\n",
            "l'erreur  l'epoch 19 est :4.253977183705055\n",
            "l'erreur  l'epoch 20 est :4.182704751734117\n",
            "l'erreur  l'epoch 21 est :4.140627599194234\n",
            "l'erreur  l'epoch 22 est :4.024080870643168\n",
            "l'erreur  l'epoch 23 est :3.8909625126921195\n",
            "l'erreur  l'epoch 24 est :3.855524635215637\n",
            "l'erreur  l'epoch 25 est :3.744803833638082\n",
            "l'erreur  l'epoch 26 est :3.6721753882782617\n",
            "l'erreur  l'epoch 27 est :3.678933418472737\n",
            "l'erreur  l'epoch 28 est :3.560105824196897\n",
            "l'erreur  l'epoch 29 est :3.431430552147481\n",
            "l'erreur  l'epoch 30 est :3.5090547831720036\n",
            "l'erreur  l'epoch 31 est :3.3756208392885982\n",
            "l'erreur  l'epoch 32 est :3.2859874744143918\n",
            "l'erreur  l'epoch 33 est :3.323906884788499\n",
            "l'erreur  l'epoch 34 est :3.2539351903240514\n",
            "l'erreur  l'epoch 35 est :3.2291252852136787\n",
            "l'erreur  l'epoch 36 est :3.1692338680414713\n",
            "l'erreur  l'epoch 37 est :3.080274806274051\n",
            "l'erreur  l'epoch 38 est :3.0343722019412263\n",
            "l'erreur  l'epoch 39 est :3.0579355082488022\n",
            "l'erreur  l'epoch 40 est :3.066126670628567\n",
            "l'erreur  l'epoch 41 est :3.0058659730993407\n",
            "l'erreur  l'epoch 42 est :2.943023029807663\n",
            "l'erreur  l'epoch 43 est :2.9196602747280953\n",
            "l'erreur  l'epoch 44 est :2.891594769565503\n",
            "l'erreur  l'epoch 45 est :2.828472974990435\n",
            "l'erreur  l'epoch 46 est :2.8146062997815564\n",
            "l'erreur  l'epoch 47 est :2.8245711663113333\n",
            "l'erreur  l'epoch 48 est :2.7122497673701744\n",
            "l'erreur  l'epoch 49 est :2.7836586404242745\n",
            "l'erreur  l'epoch 50 est :2.7317891180937504\n",
            "l'erreur  l'epoch 51 est :2.6447287528910928\n",
            "l'erreur  l'epoch 52 est :2.692486647795803\n",
            "l'erreur  l'epoch 53 est :2.6627174691483155\n",
            "l'erreur  l'epoch 54 est :2.6492606135343673\n",
            "l'erreur  l'epoch 55 est :2.5462127366151965\n",
            "l'erreur  l'epoch 56 est :2.597644833416809\n",
            "l'erreur  l'epoch 57 est :2.576479992153282\n",
            "l'erreur  l'epoch 58 est :2.588795542945689\n",
            "l'erreur  l'epoch 59 est :2.4804272669968177\n",
            "l'erreur  l'epoch 60 est :2.4989964509034235\n",
            "l'erreur  l'epoch 61 est :2.437878453571872\n",
            "l'erreur  l'epoch 62 est :2.4985881836848063\n",
            "l'erreur  l'epoch 63 est :2.3992558622762155\n",
            "l'erreur  l'epoch 64 est :2.412237919522685\n",
            "l'erreur  l'epoch 65 est :2.399653407065965\n",
            "l'erreur  l'epoch 66 est :2.370573671714731\n",
            "l'erreur  l'epoch 67 est :2.409846363509847\n",
            "l'erreur  l'epoch 68 est :2.3161756120209747\n",
            "l'erreur  l'epoch 69 est :2.34035029980428\n",
            "l'erreur  l'epoch 70 est :2.319569200898089\n",
            "l'erreur  l'epoch 71 est :2.2978215224245453\n",
            "l'erreur  l'epoch 72 est :2.3085082856977475\n",
            "l'erreur  l'epoch 73 est :2.253488229299122\n",
            "l'erreur  l'epoch 74 est :2.22048331681157\n",
            "l'erreur  l'epoch 75 est :2.301137323287769\n",
            "l'erreur  l'epoch 76 est :2.2307311380086885\n",
            "l'erreur  l'epoch 77 est :2.2057567776933236\n",
            "l'erreur  l'epoch 78 est :2.187602417208244\n",
            "l'erreur  l'epoch 79 est :2.144158858681874\n",
            "l'erreur  l'epoch 80 est :2.1654025976487854\n",
            "l'erreur  l'epoch 81 est :2.1622016640971635\n",
            "l'erreur  l'epoch 82 est :2.168882347588387\n",
            "l'erreur  l'epoch 83 est :2.135514976652573\n",
            "l'erreur  l'epoch 84 est :2.1377225589645743\n",
            "l'erreur  l'epoch 85 est :2.121488033194765\n",
            "l'erreur  l'epoch 86 est :2.118848607748057\n",
            "l'erreur  l'epoch 87 est :2.127030457153527\n",
            "l'erreur  l'epoch 88 est :2.079353437576076\n",
            "l'erreur  l'epoch 89 est :2.1244473875089875\n",
            "l'erreur  l'epoch 90 est :2.05659627568216\n",
            "l'erreur  l'epoch 91 est :2.06816546223167\n",
            "l'erreur  l'epoch 92 est :2.033245471539825\n",
            "l'erreur  l'epoch 93 est :2.0262542077982317\n",
            "l'erreur  l'epoch 94 est :2.132436146201749\n",
            "l'erreur  l'epoch 95 est :2.0318268347797748\n",
            "l'erreur  l'epoch 96 est :2.0472258920636652\n",
            "l'erreur  l'epoch 97 est :2.058351161864178\n",
            "l'erreur  l'epoch 98 est :2.0704045059192624\n",
            "l'erreur  l'epoch 99 est :1.985899112436356\n",
            "l'erreur  l'epoch 100 est :2.016563672505985\n",
            "l'erreur  l'epoch 101 est :1.9834070832865967\n",
            "l'erreur  l'epoch 102 est :1.989320249840388\n",
            "l'erreur  l'epoch 103 est :2.0145135652208386\n",
            "l'erreur  l'epoch 104 est :1.9362792343809316\n",
            "l'erreur  l'epoch 105 est :1.9419121101220191\n",
            "l'erreur  l'epoch 106 est :2.03782136245616\n",
            "l'erreur  l'epoch 107 est :1.9608356297408855\n",
            "l'erreur  l'epoch 108 est :1.9566903560086137\n",
            "l'erreur  l'epoch 109 est :1.970680247537179\n",
            "l'erreur  l'epoch 110 est :1.9070551370420632\n",
            "l'erreur  l'epoch 111 est :1.9065557471420227\n",
            "l'erreur  l'epoch 112 est :1.9227612497643307\n",
            "l'erreur  l'epoch 113 est :1.9303037071995028\n",
            "l'erreur  l'epoch 114 est :1.9418925907136175\n",
            "l'erreur  l'epoch 115 est :1.8988038508088956\n",
            "l'erreur  l'epoch 116 est :1.9182945952165515\n",
            "l'erreur  l'epoch 117 est :1.8702428294889222\n",
            "l'erreur  l'epoch 118 est :1.9330618662034096\n",
            "l'erreur  l'epoch 119 est :1.8833400241182636\n",
            "l'erreur  l'epoch 120 est :1.8688556967972616\n",
            "l'erreur  l'epoch 121 est :1.8491500257147548\n",
            "l'erreur  l'epoch 122 est :1.8143850033477547\n",
            "l'erreur  l'epoch 123 est :1.8954110209490302\n",
            "l'erreur  l'epoch 124 est :1.8589181461219846\n",
            "l'erreur  l'epoch 125 est :1.8321847579570654\n",
            "l'erreur  l'epoch 126 est :1.8281209363546609\n",
            "l'erreur  l'epoch 127 est :1.880708354121243\n",
            "l'erreur  l'epoch 128 est :1.8288425597145208\n",
            "l'erreur  l'epoch 129 est :1.8031086545135777\n",
            "l'erreur  l'epoch 130 est :1.7679158656734222\n",
            "l'erreur  l'epoch 131 est :1.7941393819108342\n",
            "l'erreur  l'epoch 132 est :1.8464024266889472\n",
            "l'erreur  l'epoch 133 est :1.7830157607916062\n",
            "l'erreur  l'epoch 134 est :1.8077668552640653\n",
            "l'erreur  l'epoch 135 est :1.7904266956762815\n",
            "l'erreur  l'epoch 136 est :1.7693979638985333\n",
            "l'erreur  l'epoch 137 est :1.8257079330678188\n",
            "l'erreur  l'epoch 138 est :1.723609388470986\n",
            "l'erreur  l'epoch 139 est :1.7794802602512994\n",
            "l'erreur  l'epoch 140 est :1.7303937565636307\n",
            "l'erreur  l'epoch 141 est :1.7584577752278974\n",
            "l'erreur  l'epoch 142 est :1.8058520947547718\n",
            "l'erreur  l'epoch 143 est :1.7549239640602627\n",
            "l'erreur  l'epoch 144 est :1.7522940126156596\n",
            "l'erreur  l'epoch 145 est :1.7570501803941236\n",
            "l'erreur  l'epoch 146 est :1.8243924473613782\n",
            "l'erreur  l'epoch 147 est :1.72708763087703\n",
            "l'erreur  l'epoch 148 est :1.7382896065700757\n",
            "l'erreur  l'epoch 149 est :1.731409838464553\n",
            "l'erreur  l'epoch 150 est :1.7117373154919175\n",
            "l'erreur  l'epoch 151 est :1.7134994904080927\n",
            "l'erreur  l'epoch 152 est :1.7344432036680044\n",
            "l'erreur  l'epoch 153 est :1.697813829272612\n",
            "l'erreur  l'epoch 154 est :1.7214532113375787\n",
            "l'erreur  l'epoch 155 est :1.683741445248621\n",
            "l'erreur  l'epoch 156 est :1.6711406857823425\n",
            "l'erreur  l'epoch 157 est :1.6888911950825376\n",
            "l'erreur  l'epoch 158 est :1.7140228229643077\n",
            "l'erreur  l'epoch 159 est :1.6958066050759992\n",
            "l'erreur  l'epoch 160 est :1.7070738415497977\n",
            "l'erreur  l'epoch 161 est :1.7184144142586262\n",
            "l'erreur  l'epoch 162 est :1.6670271676153527\n",
            "l'erreur  l'epoch 163 est :1.686593475982729\n",
            "l'erreur  l'epoch 164 est :1.658373412544441\n",
            "l'erreur  l'epoch 165 est :1.657199782170863\n",
            "l'erreur  l'epoch 166 est :1.6667835114423386\n",
            "l'erreur  l'epoch 167 est :1.65974762324459\n",
            "l'erreur  l'epoch 168 est :1.6826163983171172\n",
            "l'erreur  l'epoch 169 est :1.6851848398050966\n",
            "l'erreur  l'epoch 170 est :1.6961546794560869\n",
            "l'erreur  l'epoch 171 est :1.6642042128417098\n",
            "l'erreur  l'epoch 172 est :1.6430483671490619\n",
            "l'erreur  l'epoch 173 est :1.653662702775959\n",
            "l'erreur  l'epoch 174 est :1.6073081178324011\n",
            "l'erreur  l'epoch 175 est :1.6871238994723743\n",
            "l'erreur  l'epoch 176 est :1.6768117425651257\n",
            "l'erreur  l'epoch 177 est :1.6187417313992327\n",
            "l'erreur  l'epoch 178 est :1.6777539102778833\n",
            "l'erreur  l'epoch 179 est :1.6399088308097785\n",
            "l'erreur  l'epoch 180 est :1.6190619770759627\n",
            "l'erreur  l'epoch 181 est :1.686896424417726\n",
            "l'erreur  l'epoch 182 est :1.6212857815427382\n",
            "l'erreur  l'epoch 183 est :1.6280393807631826\n",
            "l'erreur  l'epoch 184 est :1.58536213363392\n",
            "l'erreur  l'epoch 185 est :1.618139588753211\n",
            "l'erreur  l'epoch 186 est :1.5820434692438587\n",
            "l'erreur  l'epoch 187 est :1.5862438925162794\n",
            "l'erreur  l'epoch 188 est :1.5895801472228128\n",
            "l'erreur  l'epoch 189 est :1.5950660464530226\n",
            "l'erreur  l'epoch 190 est :1.5768861848212872\n",
            "l'erreur  l'epoch 191 est :1.6041489449441046\n",
            "l'erreur  l'epoch 192 est :1.6053520320360952\n",
            "l'erreur  l'epoch 193 est :1.5985003076516326\n",
            "l'erreur  l'epoch 194 est :1.606614086746006\n",
            "l'erreur  l'epoch 195 est :1.6236723142289575\n",
            "l'erreur  l'epoch 196 est :1.5976083627670021\n",
            "l'erreur  l'epoch 197 est :1.5951263258078514\n",
            "l'erreur  l'epoch 198 est :1.57796883008639\n",
            "l'erreur  l'epoch 199 est :1.5627673968354239\n",
            "l'erreur  l'epoch 200 est :1.6012281286970549\n",
            "l'erreur  l'epoch 201 est :1.5831980452141412\n",
            "l'erreur  l'epoch 202 est :1.5874255271498356\n",
            "l'erreur  l'epoch 203 est :1.5685566285647627\n",
            "l'erreur  l'epoch 204 est :1.5512542979595878\n",
            "l'erreur  l'epoch 205 est :1.5738384321030352\n",
            "l'erreur  l'epoch 206 est :1.597906573851685\n",
            "l'erreur  l'epoch 207 est :1.5468402745368488\n",
            "l'erreur  l'epoch 208 est :1.6076155492592104\n",
            "l'erreur  l'epoch 209 est :1.5645206621394139\n",
            "l'erreur  l'epoch 210 est :1.5212372630941262\n",
            "l'erreur  l'epoch 211 est :1.5626480018637467\n",
            "l'erreur  l'epoch 212 est :1.5457658273040558\n",
            "l'erreur  l'epoch 213 est :1.5914249373930365\n",
            "l'erreur  l'epoch 214 est :1.5439214460643762\n",
            "l'erreur  l'epoch 215 est :1.5363009048229894\n",
            "l'erreur  l'epoch 216 est :1.6016877670227172\n",
            "l'erreur  l'epoch 217 est :1.5729133013102679\n",
            "l'erreur  l'epoch 218 est :1.5795514729675886\n",
            "l'erreur  l'epoch 219 est :1.5467488783366916\n",
            "l'erreur  l'epoch 220 est :1.5473191963219544\n",
            "l'erreur  l'epoch 221 est :1.5327875330098872\n",
            "l'erreur  l'epoch 222 est :1.5426427164237022\n",
            "l'erreur  l'epoch 223 est :1.528928601236055\n",
            "l'erreur  l'epoch 224 est :1.5315882234355156\n",
            "l'erreur  l'epoch 225 est :1.5406915285586098\n",
            "l'erreur  l'epoch 226 est :1.5677951936967331\n",
            "l'erreur  l'epoch 227 est :1.5326554225619273\n",
            "l'erreur  l'epoch 228 est :1.5476917895892124\n",
            "l'erreur  l'epoch 229 est :1.5466979413323234\n",
            "l'erreur  l'epoch 230 est :1.5163889965768422\n",
            "l'erreur  l'epoch 231 est :1.5508460525793184\n",
            "l'erreur  l'epoch 232 est :1.5601088936475065\n",
            "l'erreur  l'epoch 233 est :1.5630418960828467\n",
            "l'erreur  l'epoch 234 est :1.5019488675151003\n",
            "l'erreur  l'epoch 235 est :1.543468681215408\n",
            "l'erreur  l'epoch 236 est :1.5402496229736182\n",
            "l'erreur  l'epoch 237 est :1.5008357685085658\n",
            "l'erreur  l'epoch 238 est :1.5174297904138887\n",
            "l'erreur  l'epoch 239 est :1.5100442778776755\n",
            "l'erreur  l'epoch 240 est :1.473602606057663\n",
            "l'erreur  l'epoch 241 est :1.4938382425890788\n",
            "l'erreur  l'epoch 242 est :1.512410644205225\n",
            "l'erreur  l'epoch 243 est :1.551204875491152\n",
            "l'erreur  l'epoch 244 est :1.5266824698573902\n",
            "l'erreur  l'epoch 245 est :1.4780242784526632\n",
            "l'erreur  l'epoch 246 est :1.5014558287851203\n",
            "l'erreur  l'epoch 247 est :1.4872562652932495\n",
            "l'erreur  l'epoch 248 est :1.5203181009077322\n",
            "l'erreur  l'epoch 249 est :1.4814992841527792\n",
            "l'erreur  l'epoch 250 est :1.5080158293064068\n",
            "l'erreur  l'epoch 251 est :1.4887993695266861\n",
            "l'erreur  l'epoch 252 est :1.517502304053658\n",
            "l'erreur  l'epoch 253 est :1.4789924991317736\n",
            "l'erreur  l'epoch 254 est :1.4669879131711339\n",
            "l'erreur  l'epoch 255 est :1.4794862022216804\n",
            "l'erreur  l'epoch 256 est :1.476286940900948\n",
            "l'erreur  l'epoch 257 est :1.4559033535818828\n",
            "l'erreur  l'epoch 258 est :1.4562808362770137\n",
            "l'erreur  l'epoch 259 est :1.4608606888103413\n",
            "l'erreur  l'epoch 260 est :1.4671797154472621\n",
            "l'erreur  l'epoch 261 est :1.4695041955107198\n",
            "l'erreur  l'epoch 262 est :1.4747336552567216\n",
            "l'erreur  l'epoch 263 est :1.508699179763507\n",
            "l'erreur  l'epoch 264 est :1.4876680299199483\n",
            "l'erreur  l'epoch 265 est :1.4307651249896112\n",
            "l'erreur  l'epoch 266 est :1.481011195730942\n",
            "l'erreur  l'epoch 267 est :1.449950278093951\n",
            "l'erreur  l'epoch 268 est :1.4389877928410972\n",
            "l'erreur  l'epoch 269 est :1.4975401743016805\n",
            "l'erreur  l'epoch 270 est :1.4417452381793598\n",
            "l'erreur  l'epoch 271 est :1.4714641957682524\n",
            "l'erreur  l'epoch 272 est :1.467757171602801\n",
            "l'erreur  l'epoch 273 est :1.431533231430321\n",
            "l'erreur  l'epoch 274 est :1.468447417501297\n",
            "l'erreur  l'epoch 275 est :1.488311396273682\n",
            "l'erreur  l'epoch 276 est :1.5020147517236464\n",
            "l'erreur  l'epoch 277 est :1.4600243862447138\n",
            "l'erreur  l'epoch 278 est :1.4541150527384352\n",
            "l'erreur  l'epoch 279 est :1.4602334060710427\n",
            "l'erreur  l'epoch 280 est :1.4690518380999138\n",
            "l'erreur  l'epoch 281 est :1.4730516470502373\n",
            "l'erreur  l'epoch 282 est :1.4220943497649599\n",
            "l'erreur  l'epoch 283 est :1.4397656714444174\n",
            "l'erreur  l'epoch 284 est :1.4606976144761705\n",
            "l'erreur  l'epoch 285 est :1.4076767835433122\n",
            "l'erreur  l'epoch 286 est :1.4416631983582116\n",
            "l'erreur  l'epoch 287 est :1.4369522577000116\n",
            "l'erreur  l'epoch 288 est :1.445675471605643\n",
            "l'erreur  l'epoch 289 est :1.462537227469045\n",
            "l'erreur  l'epoch 290 est :1.4533330907913025\n",
            "l'erreur  l'epoch 291 est :1.459113990604724\n",
            "l'erreur  l'epoch 292 est :1.4340308787348113\n",
            "l'erreur  l'epoch 293 est :1.4522415141389775\n",
            "l'erreur  l'epoch 294 est :1.4308599535665145\n",
            "l'erreur  l'epoch 295 est :1.4258835389877922\n",
            "l'erreur  l'epoch 296 est :1.4401797877202083\n",
            "l'erreur  l'epoch 297 est :1.4098494149491791\n",
            "l'erreur  l'epoch 298 est :1.4088632090448294\n",
            "l'erreur  l'epoch 299 est :1.4445386436748593\n"
          ]
        }
      ],
      "source": [
        "dbn =  DBN([p, 100, 100, q])\n",
        "error = dbn.train_DBN(X_train, epsilon=0.1, batch_size=32, nb_epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABecAAAE3CAYAAAAg6vr5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN4ElEQVR4nO3awZKbSBBFUeUE///LOQsvHN6oC5t6RcE52yFEqgfS6AbV3f0BAAAAAABi/ls9AAAAAAAAvI04DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABB2jB5YVTPnABjW3V//u30F3IV9BezCvgJ2YV8Bu/hpX30+3pwHAAAAAIA4cR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAICwY/UAAMC9dPeyc1fVsnNDwtPur5Xf5/OxM4A5RnebHQTAv/LmPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhB2rBwDgvrp7+NiqWnr+UTPmfJqr/0Yz/j8CfD7j+8XuB2bwrAo80Q6/3560K705DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEHasH4Bm6e/UIP6qq1SPAdmbcNzP2xZvv7x32L7yB3QZwb6ufmex0YIbR3TZjB9lr1/DmPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhB2rB+C+unv1CJc6832qauIkMM/odT7jGl957qd52v6FN3jrbnvr9wYAOGvl77wZ59YAruHNeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACDtWD0Bed68eAZikql55bu7NtQEA9zH6e9C/38ATaWLX8e/ENbw5DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEHasHgJSqGj62uy//TOCZRvcFwBPN2IGer+C8M/fiynvMcxMwY1/ZLdfyLJblzXkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAg7Vg/Au1TV6hGG7DIn8Et3Dx87en+f+cyVZnwfOxAA7mHl88guz0IreWaCuXbYQzP2gN9u7+LNeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAg7Fg9ALxFd1/6eVV16edBytX3wurz73Iv7jIn8Nvovhq9v8/sPzsD5nnas9AT2YHwXDvc3zvMyHW8OQ8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHH6gHIq6qh47p78iRZu3yf0f8/cDe73GMAT2P/wl7OPO+7v3/m9xPcg30Ff8eb8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABA2LF6AN6lu1ePsExVrR4BmODMXhvdA6t3pX0F86y+v0ftMifsaJf7a+XzwC5/I2A/T9svfrvtz5vzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQdqwegPuqquFju3viJMDVnnbPzthXZz7z6nMD93Dmnp2xM57G3wh+2eUZY+Vzi30BMGbl71uu4c15AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIO1YPwDW6e/jYqrr8M99q9G8JKaP37Yxr9607Y5fvbV/BeWfum112AfDLyt9PM/5NnrGv7MD3mXFf8B5aE/wdb84DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHiPAAAAAAAhInzAAAAAAAQJs4DAAAAAECYOA8AAAAAAGHH6gHI6+6h46rq8s/cxZnvDney8todPfeZfbHDvfjmXQn8NmNf2Rkwz4x/v3d4btnF054XP5+115FrmDtZvX89X3E33pwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAIAwcR4AAAAAAMLEeQAAAAAACBPnAQAAAAAgTJwHAAAAAICwY/UAfNfdrzw38FxV9erzA++2y/OVXQm/vfV+2GVf7WLldfTWa5j9vfXaPfO97er9eXMeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAg7Vg/Ad1U1dFx3X/6ZZ5w5/yozvjdwDzN20Mr9u3qnA+ft8CwEPJcdBLzZ03ag34Pv4s15AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIO1YPwHfdPXRcVS39zNFjR88NPNcue+Bpc57Z6TucG+7GsxAwg50BvNlbd6DfT+/izXkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIOxYPcAbdfdrP3OGqlo9AsCQ0b26y17bZU5gnjPPi3YGMMq+AOAtvDkPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABhx+oB3qiqho/t7omTfHdmTgB+Zq8CZ4zuDM+L8Fw77AGAM+wr+JM35wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIOxYPQDfVdXqES7V3UPHPe17A/uxh2CuHZ4JRmf8fMbnnPF97CtgF/YV7GXGs9DTnPneZ/6evIc35wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAsGP1AOR199BxVXX5uWd8JnAPo7tlNXvoZyv/neA9Rq+fGbvFtctd2b/8rTPXxA7PbHY/zLVyD8zYV7vc37vMSZY35wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIOxYPQB5VbV6BOCB7Jbn8P+SO1l5PboXSHPN8XTdfflnum/gvF3um13mhH/hzXkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIEycBwAAAACAMHEeAAAAAADCxHkAAAAAAAgT5wEAAAAAIOxYPQDfdffQcVU1eRKAa4zutc/HbgMAuNoOz1dnnhcBYGfenAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACBMnAcAAAAAgDBxHgAAAAAAwsR5AAAAAAAIE+cBAAAAACBMnAcAAAAAgLBj9QB8V1WrRwC4lL0GAMA3nhcBeAtvzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABAmzgMAAAAAQJg4DwAAAAAAYeI8AAAAAACEifMAAAAAABBW3d2rhwAAAAAAgDfx5jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAISJ8wAAAAAAECbOAwAAAABAmDgPAAAAAABh4jwAAAAAAIT9D9aUydrlkYSmAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dbn.generer_image_DBN(5,20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
